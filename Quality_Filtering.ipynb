{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01d9dd-ab62-4311-84ce-bfd1ede543c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup and Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "from collections import Counter, defaultdict\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ea6e5-f4d6-4df3-8f91-60e87cd2a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Path and Configuration\n",
    "\n",
    "BASE_PATH = r\"D:/Dream_dataset\"  # Specify the base path to your dataset here\n",
    "\n",
    "# Save all the datasets to the BASE_PATH folder\n",
    "# Path configuration based on your folder structure\n",
    "datasets_config = {\n",
    "    'APTOS2019': f'{BASE_PATH}/APTOS 2019',\n",
    "    'Diabetic_Retinopathy_V03': f'{BASE_PATH}/Diabetic Retinopathy_V03',\n",
    "    'IDRiD': f'{BASE_PATH}/IDRiD',\n",
    "    'Messidor2': f'{BASE_PATH}/Messidor 2',\n",
    "    'SUSTech_SYSU': f'{BASE_PATH}/SUSTech_SYSU',\n",
    "    'DeepDRiD': f'{BASE_PATH}/DeepDRiD/DeepDRiD/DR/Original'\n",
    "}\n",
    "\n",
    "# Other configuration settings\n",
    "OUTPUT_DIR = 'quality_review'\n",
    "RANDOM_SEED = 42\n",
    "N_SAMPLES_PER_DATASET = 300  # Number of images to sample for characterization\n",
    "\n",
    "print(\"Configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c183499-77f9-4247-bc5a-83b93c49441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Validate Dataset Paths\n",
    "\n",
    "def validate_dataset_paths(datasets_config):\n",
    "    valid_datasets = {}\n",
    "    \n",
    "    for name, path in datasets_config.items():\n",
    "        print(f\"\\nChecking {name}:\")\n",
    "        print(f\"  Path: {path}\")\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Path does not exist\")\n",
    "            continue\n",
    "            \n",
    "        # Check for DR class folders (0, 1, 2, 3, 4)\n",
    "        dr_folders = []\n",
    "        image_counts = {}\n",
    "        \n",
    "        try:\n",
    "            for item in os.listdir(path):\n",
    "                item_path = os.path.join(path, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    if item.isdigit() and int(item) in [0, 1, 2, 3, 4]:\n",
    "                        dr_class = int(item)\n",
    "                        dr_folders.append(dr_class)\n",
    "                        image_extensions = ('.jpg', '.jpeg', '.png', '.tiff', '.bmp', '.JPG', '.JPEG', '.PNG')\n",
    "                        image_count = 0\n",
    "                        for root, dirs, files in os.walk(item_path):\n",
    "                            for file in files:\n",
    "                                if file.lower().endswith(image_extensions):\n",
    "                                    image_count += 1\n",
    "                        image_counts[dr_class] = image_count\n",
    "            \n",
    "            if dr_folders:\n",
    "                dr_folders.sort()\n",
    "                total_images = sum(image_counts.values())\n",
    "                print(f\" Found DR classes: {dr_folders}\")\n",
    "                print(f\"Image counts per class:\")\n",
    "                dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "                for dr_class in dr_folders:\n",
    "                    dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "                    print(f\"     {dr_name} (Class {dr_class}): {image_counts[dr_class]:,} images\")\n",
    "                print(f\"Total images: {total_images:,}\")\n",
    "                \n",
    "                if total_images > 0:\n",
    "                    valid_datasets[name] = path\n",
    "                else:\n",
    "                    print(f\"No images found\")\n",
    "            else:\n",
    "                print(f\"No DR class folders (0,1,2,3,4) found\")\n",
    "                print(f\"Available folders: {[item for item in os.listdir(path) if os.path.isdir(os.path.join(path, item))]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing path: {e}\")\n",
    "\n",
    "    return valid_datasets\n",
    "\n",
    "# Run validation\n",
    "valid_datasets = validate_dataset_paths(datasets_config)\n",
    "\n",
    "if not valid_datasets:\n",
    "    print(\"\\nNo valid datasets found!\")\n",
    "    print(\"1. Update BASE_PATH in cell [2] to your actual dataset location\")\n",
    "    print(\"2. Ensure your datasets have folders named 0, 1, 2, 3, 4 containing images\")\n",
    "else:\n",
    "    print(f\"\\n{len(valid_datasets)} Datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b4678-122c-479c-859d-964c1a222d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Quality Identifier Class Definition\n",
    "\n",
    "class QualityIdentifier:\n",
    "    def __init__(self, output_dir='quality_review', random_seed=42):\n",
    "        self.output_dir = output_dir\n",
    "        self.dataset_profiles = {}\n",
    "        self.identification_results = []\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(f'{output_dir}/sample_images', exist_ok=True)\n",
    "        os.makedirs(f'{output_dir}/flagged_samples', exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Quality identification output directory: {output_dir}\")\n",
    "    \n",
    "    def extract_dr_label_from_path(self, image_path):\n",
    "        parts = image_path.split(os.sep)\n",
    "        \n",
    "        for part in parts:\n",
    "            if part.isdigit() and int(part) in [0, 1, 2, 3, 4]:\n",
    "                return int(part)\n",
    "        \n",
    "        dr_patterns = {\n",
    "            'no_dr': 0, 'normal': 0, 'grade_0': 0, 'class_0': 0,\n",
    "            'mild': 1, 'grade_1': 1, 'class_1': 1,\n",
    "            'moderate': 2, 'grade_2': 2, 'class_2': 2,\n",
    "            'severe': 3, 'grade_3': 3, 'class_3': 3,\n",
    "            'proliferative': 4, 'grade_4': 4, 'class_4': 4\n",
    "        }\n",
    "        \n",
    "        for part in parts:\n",
    "            part_lower = part.lower()\n",
    "            if part_lower in dr_patterns:\n",
    "                return dr_patterns[part_lower]\n",
    "        return None\n",
    "    \n",
    "    def sample_images_strategically(self, dataset_path, n_samples):\n",
    "        all_images = []\n",
    "        class_images = {0: [], 1: [], 2: [], 3: [], 4: []}\n",
    "        \n",
    "        image_extensions = ('.jpg', '.jpeg', '.png', '.tiff', '.bmp', '.JPG', '.JPEG', '.PNG')\n",
    "        \n",
    "        for root, dirs, files in os.walk(dataset_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(image_extensions):\n",
    "                    image_path = os.path.join(root, file)\n",
    "                    dr_class = self.extract_dr_label_from_path(image_path)\n",
    "                    if dr_class is not None:\n",
    "                        class_images[dr_class].append(image_path)\n",
    "        \n",
    "        for dr_class, images in class_images.items():\n",
    "            dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "            dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "            if images:\n",
    "                print(f\"    {dr_name}: {len(images)} images\")\n",
    "        \n",
    "        sampled_images = []\n",
    "        total_available = sum(len(images) for images in class_images.values())\n",
    "        \n",
    "        if total_available == 0:\n",
    "            print(\"No images found with valid DR labels\")\n",
    "            return []\n",
    "        \n",
    "        samples_per_class = max(1, n_samples // 5)\n",
    "        \n",
    "        for dr_class, images in class_images.items():\n",
    "            if images:\n",
    "                n_class_samples = min(samples_per_class, len(images))\n",
    "                sampled = np.random.choice(images, n_class_samples, replace=False)\n",
    "                sampled_images.extend(sampled)\n",
    "\n",
    "        print(f\"Selected {len(sampled_images)} stratified samples\")\n",
    "        return sampled_images\n",
    "    \n",
    "    def safe_calculate_brightness(self, image):\n",
    "        try:\n",
    "            brightness = float(np.mean(image))\n",
    "            return max(0.0, min(255.0, brightness))\n",
    "        except:\n",
    "            return 127.5\n",
    "    \n",
    "    def safe_calculate_contrast(self, image):\n",
    "        try:\n",
    "            contrast = float(np.std(image))\n",
    "            return max(0.0, contrast)\n",
    "        except:\n",
    "            return 50.0\n",
    "    \n",
    "    def safe_calculate_sharpness(self, gray):\n",
    "        try:\n",
    "            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "            return max(0.0, float(laplacian_var))\n",
    "        except:\n",
    "            return 500.0\n",
    "    \n",
    "    def safe_calculate_entropy(self, gray):\n",
    "        try:\n",
    "            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "            hist = hist.flatten()\n",
    "            hist = hist[hist > 0]\n",
    "            if len(hist) == 0:\n",
    "                return 4.0\n",
    "            prob = hist / hist.sum()\n",
    "            entropy = float(-np.sum(prob * np.log2(prob)))\n",
    "            return max(0.0, min(8.0, entropy))\n",
    "        except:\n",
    "            return 4.0\n",
    "\n",
    "    def assess_vessel_visibility_improved(self, image):\n",
    "        try:\n",
    "            green = image[:, :, 1]\n",
    "            \n",
    "            kernels = {\n",
    "                'horizontal': np.array([[-1, -1, -1], [2, 2, 2], [-1, -1, -1]], dtype=np.float32),\n",
    "                'vertical': np.array([[-1, 2, -1], [-1, 2, -1], [-1, 2, -1]], dtype=np.float32),\n",
    "                'diagonal1': np.array([[2, -1, -1], [-1, 2, -1], [-1, -1, 2]], dtype=np.float32),\n",
    "                'diagonal2': np.array([[-1, -1, 2], [-1, 2, -1], [2, -1, -1]], dtype=np.float32)\n",
    "            }\n",
    "            \n",
    "            vessel_responses = []\n",
    "            for kernel in kernels.values():\n",
    "                filtered = cv2.filter2D(green, cv2.CV_32F, kernel)\n",
    "                vessel_responses.append(filtered)\n",
    "            \n",
    "            max_response = np.maximum.reduce(vessel_responses)\n",
    "            threshold = np.percentile(max_response, 95)\n",
    "            vessel_pixels = np.sum(max_response > threshold)\n",
    "            total_pixels = max_response.shape[0] * max_response.shape[1]\n",
    "            \n",
    "            visibility_score = vessel_pixels / max(total_pixels, 1)\n",
    "            return float(min(0.1, visibility_score))\n",
    "            \n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def assess_optic_disc_visibility(self, image):\n",
    "        try:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            bright_pixels = np.sum(gray > np.percentile(gray, 90))\n",
    "            total_pixels = gray.shape[0] * gray.shape[1]\n",
    "            return float(bright_pixels / max(total_pixels, 1))\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def assess_illumination_uniformity(self, gray):\n",
    "        try:\n",
    "            h, w = gray.shape\n",
    "            regions = []\n",
    "            \n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    start_y, end_y = i * h // 3, (i + 1) * h // 3\n",
    "                    start_x, end_x = j * w // 3, (j + 1) * w // 3\n",
    "                    region = gray[start_y:end_y, start_x:end_x]\n",
    "                    regions.append(np.mean(region))\n",
    "            \n",
    "            mean_intensity = np.mean(regions)\n",
    "            if mean_intensity > 0:\n",
    "                cv_score = np.std(regions) / mean_intensity\n",
    "                return float(max(0, min(1, 1 - cv_score)))\n",
    "            return 0.0\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def detect_extreme_pixels(self, gray):\n",
    "        try:\n",
    "            very_dark = np.sum(gray < 10)\n",
    "            very_bright = np.sum(gray > 245)\n",
    "            total_pixels = gray.shape[0] * gray.shape[1]\n",
    "            return float((very_dark + very_bright) / max(total_pixels, 1))\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def assess_motion_blur(self, gray):\n",
    "        try:\n",
    "            grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "            return float(np.mean(magnitude))\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def analyze_single_image(self, image_path):\n",
    "        try:\n",
    "            if not os.path.exists(image_path):\n",
    "                logger.warning(f\"File not found: {image_path}\")\n",
    "                return None\n",
    "                \n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                logger.warning(f\"Could not read image: {image_path}\")\n",
    "                return None\n",
    "            \n",
    "            if len(image.shape) != 3:\n",
    "                logger.warning(f\"Invalid image format: {image_path}\")\n",
    "                return None\n",
    "                \n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            h, w = image.shape[:2]\n",
    "\n",
    "            if h < 100 or w < 100:\n",
    "                logger.warning(f\"Image too small ({w}x{h}): {image_path}\")\n",
    "                return None\n",
    "            \n",
    "            characteristics = {\n",
    "                'image_path': image_path,\n",
    "                'filename': os.path.basename(image_path),\n",
    "                'resolution': (w, h),\n",
    "                'file_size_mb': max(0.001, os.path.getsize(image_path) / (1024*1024)),\n",
    "                \n",
    "                'brightness': self.safe_calculate_brightness(image),\n",
    "                'contrast': self.safe_calculate_contrast(image),\n",
    "                'sharpness': self.safe_calculate_sharpness(gray),\n",
    "                'entropy': self.safe_calculate_entropy(gray),\n",
    "                \n",
    "                'color_balance': float(np.std([np.mean(image[:,:,0]), np.mean(image[:,:,1]), np.mean(image[:,:,2])])),\n",
    "                \n",
    "                'vessel_visibility': self.assess_vessel_visibility_improved(image),\n",
    "                'optic_disc_visibility': self.assess_optic_disc_visibility(image),\n",
    "                'illumination_uniformity': self.assess_illumination_uniformity(gray),\n",
    "                \n",
    "                'extreme_brightness_pixels': self.detect_extreme_pixels(gray),\n",
    "                'motion_blur_score': self.assess_motion_blur(gray)\n",
    "            }\n",
    "            return characteristics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "print(\"QualityIdentifier class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5. Parameter Validation\n",
    "\n",
    "class QualityParameters:\n",
    "    def __init__(self):\n",
    "        # Validated thresholds\n",
    "        self.BRIGHTNESS_BOUNDS = (10, 245)\n",
    "        self.BLACK_THRESHOLD = 30\n",
    "        self.VESSEL_PERCENTILE = 95\n",
    "        self.OPTIC_DISC_PERCENTILE = 90\n",
    "        \n",
    "        # Severity-specific adaptive thresholds\n",
    "        self.SEVERITY_PERCENTILES = {\n",
    "            0: 15,  # No DR - strictest\n",
    "            1: 12,  # Mild DR\n",
    "            2: 10,  # Moderate DR\n",
    "            3: 8,   # Severe DR\n",
    "            4: 5    # PDR - most relaxed\n",
    "        }\n",
    "        \n",
    "        # Quality composition weights\n",
    "        self.BASIC_WEIGHT = 0.3\n",
    "        self.MEDICAL_WEIGHT = 0.7\n",
    "        \n",
    "        # Minimum image requirements\n",
    "        self.MIN_RESOLUTION = (100, 100)\n",
    "        self.MIN_FILE_SIZE_MB = 0.001\n",
    "    \n",
    "    def validate_bounds(self):\n",
    "        assert 0 <= self.BRIGHTNESS_BOUNDS[0] < self.BRIGHTNESS_BOUNDS[1] <= 255\n",
    "        assert 0 < self.BLACK_THRESHOLD < 255\n",
    "        assert 0 < self.VESSEL_PERCENTILE <= 100\n",
    "        assert 0 < self.OPTIC_DISC_PERCENTILE <= 100\n",
    "        assert abs(self.BASIC_WEIGHT + self.MEDICAL_WEIGHT - 1.0) < 0.001\n",
    "        return True\n",
    "\n",
    "# Initialize and validate parameters\n",
    "quality_params = QualityParameters()\n",
    "quality_params.validate_bounds()\n",
    "print(\"Quality parameters validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaace56-0745-4186-ab8a-7ddd39833f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize Quality Identifier\n",
    "\n",
    "# Proceed if we have datasets are validated in above cells\n",
    "if valid_datasets:\n",
    "    identifier = QualityIdentifier(\n",
    "        output_dir=OUTPUT_DIR, \n",
    "        random_seed=RANDOM_SEED\n",
    "    )\n",
    "    print(\"QualityIdentifier initialized!\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"Cannot initialize - no valid datasets found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe32452-0520-49ce-b7b8-e312f5aeaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Dataset Characterization & Adaptive Thresholding\n",
    "\n",
    "# Characterization function\n",
    "def characterize_dataset(identifier, dataset_path, dataset_name, n_samples=300):\n",
    "    print(f\"Characterizing {dataset_name}...\")\n",
    "    \n",
    "    sample_images = identifier.sample_images_strategically(dataset_path, n_samples)\n",
    "    \n",
    "    if not sample_images:\n",
    "        print(f\"No images found in {dataset_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Analyzing {len(sample_images)} sample images...\")\n",
    "    \n",
    "    # Analyze characteristics with progress updates\n",
    "    characteristics = []\n",
    "    for i, img_path in enumerate(sample_images):\n",
    "        if i % 50 == 0 and i > 0:\n",
    "            print(f\"    Progress: {i}/{len(sample_images)} ({i/len(sample_images)*100:.1f}%)\")\n",
    "        char = identifier.analyze_single_image(img_path)\n",
    "        if char:\n",
    "            characteristics.append(char)  \n",
    "\n",
    "        # Memory management\n",
    "        if i % 100 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    if not characteristics:\n",
    "        print(f\"No valid characteristics extracted from {dataset_name}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Analyzed {len(characteristics)} valid images\")\n",
    "\n",
    "    # Calculate dataset profile\n",
    "    profile = calculate_dataset_profile(dataset_name, characteristics)\n",
    "    \n",
    "    # Save sample images for review\n",
    "    save_sample_images(identifier, sample_images[:20], dataset_name)\n",
    "    return profile\n",
    "\n",
    "# Calculate dataset profile\n",
    "def calculate_dataset_profile(dataset_name, characteristics):\n",
    "    profile = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'analysis_date': datetime.now().isoformat(),\n",
    "        'n_samples_analyzed': len(characteristics),\n",
    "        'characteristics_stats': {},\n",
    "        'adaptive_thresholds': {}\n",
    "    }\n",
    "    \n",
    "    numeric_keys = [key for key in characteristics[0].keys() \n",
    "                   if key not in ['image_path', 'filename', 'resolution']]\n",
    "    \n",
    "    for key in numeric_keys:\n",
    "        # Get raw values, filtering out None\n",
    "        raw_values = [char[key] for char in characteristics if char[key] is not None]\n",
    "        \n",
    "        # Convert values, handling booleans and ensuring numeric types\n",
    "        values = []\n",
    "        for val in raw_values:\n",
    "            if isinstance(val, bool):\n",
    "                values.append(float(val))  # True -> 1.0, False -> 0.0\n",
    "            elif isinstance(val, (int, float)):\n",
    "                values.append(float(val))\n",
    "            else:\n",
    "                # Skip non-numeric, non-boolean values\n",
    "                continue\n",
    "        \n",
    "        if values:\n",
    "            profile['characteristics_stats'][key] = {\n",
    "                'mean': float(np.mean(values)),\n",
    "                'std': float(np.std(values)),\n",
    "                'min': float(np.min(values)),\n",
    "                'max': float(np.max(values)),\n",
    "                'percentiles': {\n",
    "                    '5': float(np.percentile(values, 5)),\n",
    "                    '10': float(np.percentile(values, 10)),\n",
    "                    '25': float(np.percentile(values, 25)),\n",
    "                    '50': float(np.percentile(values, 50)),\n",
    "                    '75': float(np.percentile(values, 75)),\n",
    "                    '90': float(np.percentile(values, 90)),\n",
    "                    '95': float(np.percentile(values, 95))\n",
    "                }\n",
    "            }\n",
    "\n",
    "    # Adaptive removal thresholds calculation with 3-component scoring\n",
    "    removal_percentiles = {\n",
    "        0: 15,\n",
    "        1: 12,\n",
    "        2: 10,\n",
    "        3: 8,\n",
    "        4: 5\n",
    "    }\n",
    "    \n",
    "    # Calculate combined quality scores for threshold calculation using 3-component formula\n",
    "    quality_scores = []\n",
    "    for char in characteristics:\n",
    "        # Safely get and normalize metrics with error handling\n",
    "        try:\n",
    "            # Basic Quality metrics\n",
    "            brightness = float(char['brightness']) if char['brightness'] is not None else 127.5\n",
    "            contrast = float(char['contrast']) if char['contrast'] is not None else 50.0\n",
    "            sharpness = float(char['sharpness']) if char['sharpness'] is not None else 500.0\n",
    "            entropy = float(char['entropy']) if char['entropy'] is not None else 4.0\n",
    "            \n",
    "            # Medical Quality metrics\n",
    "            illumination_uniformity = float(char['illumination_uniformity']) if char['illumination_uniformity'] is not None else 0.5\n",
    "            vessel_visibility = float(char['vessel_visibility']) if char['vessel_visibility'] is not None else 0.1\n",
    "            optic_disc_visibility = float(char['optic_disc_visibility']) if char['optic_disc_visibility'] is not None else 0.1\n",
    "            \n",
    "            # Technical Quality metrics\n",
    "            extreme_pixels = float(char['extreme_brightness_pixels']) if char['extreme_brightness_pixels'] is not None else 0.1\n",
    "            motion_blur = float(char['motion_blur_score']) if char['motion_blur_score'] is not None else 20.0\n",
    "            color_balance = float(char['color_balance']) if char['color_balance'] is not None else 15.0\n",
    "            \n",
    "            # Normalize Basic Quality metrics\n",
    "            brightness_norm = min(1.0, max(0.0, brightness / 255.0))\n",
    "            contrast_norm = min(1.0, max(0.0, contrast / 100.0))\n",
    "            sharpness_norm = min(1.0, max(0.0, sharpness / 1000.0))\n",
    "            entropy_norm = min(1.0, max(0.0, entropy / 8.0))\n",
    "            \n",
    "            basic_quality = np.mean([brightness_norm, contrast_norm, sharpness_norm, entropy_norm])\n",
    "            \n",
    "            # Normalize Medical Quality metrics\n",
    "            medical_quality = np.mean([\n",
    "                illumination_uniformity,\n",
    "                min(1.0, vessel_visibility * 10),\n",
    "                min(1.0, optic_disc_visibility * 10)\n",
    "            ])\n",
    "            \n",
    "            # Normalize Technical Quality metrics\n",
    "            extreme_pixels_norm = max(0, min(1, 1 - (extreme_pixels * 2)))\n",
    "            motion_blur_norm = min(1.0, max(0.0, motion_blur / 50.0))\n",
    "            color_balance_norm = max(0, min(1, 1 - (color_balance / 50.0)))\n",
    "            \n",
    "            technical_quality = np.mean([extreme_pixels_norm, motion_blur_norm, color_balance_norm])\n",
    "            \n",
    "            # COMPOSITE FORMULA: CQ = 0.25 × BasicQuality + 0.55 × MedicalQuality + 0.20 × TechnicalQuality\n",
    "            combined_quality = 0.25 * basic_quality + 0.55 * medical_quality + 0.20 * technical_quality\n",
    "            quality_scores.append(combined_quality)\n",
    "            \n",
    "        except (TypeError, ValueError) as e:\n",
    "            print(f\"Warning: Error calculating quality score for image, using default: {e}\")\n",
    "            quality_scores.append(0.3)\n",
    "    \n",
    "    # Set percentile-based thresholds\n",
    "    for dr_severity, percentile in removal_percentiles.items():\n",
    "        try:\n",
    "            threshold = np.percentile(quality_scores, percentile) if quality_scores else 0.3\n",
    "            profile['adaptive_thresholds'][dr_severity] = float(threshold)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error calculating threshold for DR severity {dr_severity}: {e}\")\n",
    "            profile['adaptive_thresholds'][dr_severity] = 0.3\n",
    "\n",
    "    return profile  \n",
    "\n",
    "# Save sample images for visual review\n",
    "def save_sample_images(identifier, sample_paths, dataset_name):\n",
    "    sample_dir = f'{identifier.output_dir}/sample_images/{dataset_name}'\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    \n",
    "    copied_count = 0\n",
    "    for i, img_path in enumerate(sample_paths):\n",
    "        try:\n",
    "            dst_path = f'{sample_dir}/sample_{i:02d}_{os.path.basename(img_path)}'\n",
    "            shutil.copy2(img_path, dst_path)\n",
    "            copied_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error copying sample {img_path}: {e}\")\n",
    "\n",
    "    print(f\"Saved {copied_count} sample images to {sample_dir}\")\n",
    "\n",
    "# Run characterization for all valid datasets\n",
    "if valid_datasets:\n",
    "    dataset_profiles = {}\n",
    "    \n",
    "    for dataset_name, dataset_path in valid_datasets.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        profile = characterize_dataset(identifier, dataset_path, dataset_name, N_SAMPLES_PER_DATASET)\n",
    "        if profile:\n",
    "            dataset_profiles[dataset_name] = profile\n",
    "            identifier.dataset_profiles[dataset_name] = profile\n",
    "            print(f\"{dataset_name} characterized\")\n",
    "\n",
    "            # Show key metrics\n",
    "            stats = profile['characteristics_stats']\n",
    "            print(f\"Key metrics (mean ± std):\")\n",
    "            if 'brightness' in stats:\n",
    "                print(f\"      Brightness: {stats['brightness']['mean']:.1f} ± {stats['brightness']['std']:.1f}\")\n",
    "            if 'sharpness' in stats:\n",
    "                print(f\"      Sharpness: {stats['sharpness']['mean']:.1f} ± {stats['sharpness']['std']:.1f}\")\n",
    "            if 'illumination_uniformity' in stats:\n",
    "                print(f\"      Illumination uniformity: {stats['illumination_uniformity']['mean']:.3f} ± {stats['illumination_uniformity']['std']:.3f}\")\n",
    "        else:\n",
    "            print(f\"Failed characterization {dataset_name}\")\n",
    "\n",
    "    print(f\"\\nCharacterization complete! Profiles created for {len(dataset_profiles)} datasets.\")\n",
    "else:\n",
    "    print(\"Skipping characterization - no valid datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe4d72-7516-431c-90d0-23b9e1290811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Quality Issue Identification - Updated with 3-Component Scoring\n",
    "\n",
    "def assess_image_quality_corrected(characteristics, profile, dr_severity):\n",
    "    \"\"\"Updated quality assessment with 3-component scoring system\"\"\"\n",
    "    \n",
    "    char_stats = profile['characteristics_stats']\n",
    "    normalized_scores = {}\n",
    "    \n",
    "    # Dataset-relative normalization for technical metrics (BasicQuality)\n",
    "    technical_metrics = ['brightness', 'contrast', 'sharpness', 'entropy']\n",
    "    for metric in technical_metrics:\n",
    "        if metric in char_stats and metric in characteristics:\n",
    "            stats = char_stats[metric]\n",
    "            value = characteristics[metric]\n",
    "            \n",
    "            if stats['std'] > 0:\n",
    "                z_score = (value - stats['mean']) / stats['std']\n",
    "                normalized_scores[metric] = max(0, min(1, (z_score + 3) / 6))\n",
    "            else:\n",
    "                normalized_scores[metric] = 0.5\n",
    "    \n",
    "    # Direct normalization for medical metrics (MedicalQuality) with validated bounds\n",
    "    medical_metrics = {\n",
    "        'illumination_uniformity': lambda x: min(1.0, max(0.0, x)),\n",
    "        'vessel_visibility': lambda x: min(1.0, max(0.0, x * 100)),\n",
    "        'optic_disc_visibility': lambda x: min(1.0, max(0.0, x * 50))\n",
    "    }\n",
    "    \n",
    "    for metric, normalizer in medical_metrics.items():\n",
    "        if metric in characteristics:\n",
    "            normalized_scores[metric] = normalizer(characteristics[metric])\n",
    "    \n",
    "    # NEW: Normalization for technical quality metrics (TechnicalQuality)\n",
    "    # Extreme pixels normalization (lower is better, so invert)\n",
    "    if 'extreme_brightness_pixels' in characteristics:\n",
    "        ep_value = characteristics['extreme_brightness_pixels']\n",
    "        # Invert so that fewer extreme pixels = higher score\n",
    "        normalized_scores['extreme_brightness_pixels'] = max(0, min(1, 1 - (ep_value * 2)))\n",
    "    \n",
    "    # Motion blur normalization (higher motion blur score is better)\n",
    "    if 'motion_blur_score' in characteristics:\n",
    "        mb_value = characteristics['motion_blur_score']\n",
    "        # Normalize to reasonable range (0-100 typical for gradient magnitude)\n",
    "        normalized_scores['motion_blur_score'] = min(1.0, max(0.0, mb_value / 50.0))\n",
    "    \n",
    "    # Color balance normalization (lower standard deviation is better, so invert)\n",
    "    if 'color_balance' in characteristics:\n",
    "        cb_value = characteristics['color_balance']\n",
    "        # Invert so that better color balance (lower std) = higher score\n",
    "        # Typical color balance std ranges from 0-50\n",
    "        normalized_scores['color_balance'] = max(0, min(1, 1 - (cb_value / 50.0)))\n",
    "    \n",
    "    # Calculate composite scores with NEW 3-component system\n",
    "    basic_metrics = ['brightness', 'contrast', 'sharpness', 'entropy']\n",
    "    medical_metrics_list = ['illumination_uniformity', 'vessel_visibility', 'optic_disc_visibility']\n",
    "    technical_metrics_list = ['extreme_brightness_pixels', 'motion_blur_score', 'color_balance']\n",
    "    \n",
    "    basic_score = np.mean([normalized_scores.get(m, 0.5) for m in basic_metrics])\n",
    "    medical_score = np.mean([normalized_scores.get(m, 0.5) for m in medical_metrics_list])\n",
    "    technical_score = np.mean([normalized_scores.get(m, 0.5) for m in technical_metrics_list])\n",
    "    \n",
    "    # NEW COMPOSITE FORMULA: CQ = 0.25 × BasicQuality + 0.55 × MedicalQuality + 0.20 × TechnicalQuality\n",
    "    overall_score = 0.25 * basic_score + 0.55 * medical_score + 0.20 * technical_score\n",
    "    \n",
    "    threshold = profile['adaptive_thresholds'].get(dr_severity, 0.3)\n",
    "    \n",
    "    # Enhanced removal criteria (keeping existing logic)\n",
    "    removal_reasons = []\n",
    "    \n",
    "    # Critical quality checks\n",
    "    if characteristics['sharpness'] < char_stats.get('sharpness', {}).get('percentiles', {}).get('5', 0):\n",
    "        removal_reasons.append('extremely_blurry')\n",
    "    \n",
    "    if characteristics['brightness'] < 20 or characteristics['brightness'] > 240:\n",
    "        removal_reasons.append('extreme_brightness')\n",
    "    \n",
    "    if characteristics['illumination_uniformity'] < 0.1:\n",
    "        removal_reasons.append('poor_illumination')\n",
    "    \n",
    "    if characteristics['vessel_visibility'] < char_stats.get('vessel_visibility', {}).get('percentiles', {}).get('10', 0):\n",
    "        removal_reasons.append('poor_vessel_visibility')\n",
    "    \n",
    "    if characteristics['extreme_brightness_pixels'] > 0.3:\n",
    "        removal_reasons.append('too_many_extreme_pixels')\n",
    "    \n",
    "    if characteristics['file_size_mb'] < 0.1:\n",
    "        removal_reasons.append('file_too_small')\n",
    "    \n",
    "    w, h = characteristics['resolution']\n",
    "    if w < 224 or h < 224:\n",
    "        removal_reasons.append('resolution_too_low')\n",
    "    \n",
    "    # NEW: Additional technical quality checks\n",
    "    if characteristics.get('motion_blur_score', 0) < 5:  # Very low motion blur score indicates severe blur\n",
    "        removal_reasons.append('severe_motion_blur')\n",
    "    \n",
    "    if characteristics.get('color_balance', 0) > 40:  # High color balance std indicates severe color cast\n",
    "        removal_reasons.append('severe_color_imbalance')\n",
    "    \n",
    "    # Decision logic (keeping existing logic)\n",
    "    if len(removal_reasons) >= 2:\n",
    "        action = 'REMOVE'\n",
    "        confidence = 'HIGH'\n",
    "    elif overall_score < threshold:\n",
    "        action = 'REMOVE'\n",
    "        confidence = 'MEDIUM'\n",
    "    else:\n",
    "        action = 'KEEP'\n",
    "        confidence = 'HIGH' if overall_score > threshold + 0.1 else 'MEDIUM'\n",
    "    \n",
    "    return {\n",
    "        'overall_score': overall_score,\n",
    "        'basic_score': basic_score,      # NEW: Return individual component scores\n",
    "        'medical_score': medical_score,  # NEW: Return individual component scores  \n",
    "        'technical_score': technical_score,  # NEW: Return individual component scores\n",
    "        'threshold': threshold,\n",
    "        'action': action,\n",
    "        'reasons': removal_reasons,\n",
    "        'confidence': confidence,\n",
    "        'normalized_scores': normalized_scores\n",
    "    }\n",
    "\n",
    "def identify_quality_issues(identifier, dataset_path, dataset_name, profile):\n",
    "    print(f\"Identifying quality issues in {dataset_name}\")\n",
    "    \n",
    "    results = []\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    image_extensions = ('.jpg', '.jpeg', '.png', '.tiff', '.bmp', '.JPG', '.JPEG', '.PNG')\n",
    "    \n",
    "    total_images = 0\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(image_extensions):\n",
    "                image_path = os.path.join(root, file)\n",
    "                dr_severity = identifier.extract_dr_label_from_path(image_path)\n",
    "                if dr_severity is not None:\n",
    "                    total_images += 1\n",
    "    \n",
    "    print(f\"  Found {total_images:,} images to analyze\")\n",
    "    \n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(image_extensions):\n",
    "                image_path = os.path.join(root, file)\n",
    "                dr_severity = identifier.extract_dr_label_from_path(image_path)\n",
    "                \n",
    "                if dr_severity is None:\n",
    "                    continue\n",
    "                \n",
    "                characteristics = identifier.analyze_single_image(image_path)\n",
    "                if not characteristics:\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                quality_assessment = assess_image_quality_corrected(characteristics, profile, dr_severity)\n",
    "                \n",
    "                result = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'image_path': image_path,\n",
    "                    'filename': file,\n",
    "                    'dr_severity': dr_severity,\n",
    "                    'overall_quality_score': quality_assessment['overall_score'],\n",
    "                    'basic_quality_score': quality_assessment['basic_score'],      # NEW\n",
    "                    'medical_quality_score': quality_assessment['medical_score'],  # NEW\n",
    "                    'technical_quality_score': quality_assessment['technical_score'],  # NEW\n",
    "                    'threshold_used': quality_assessment['threshold'],\n",
    "                    'recommended_action': quality_assessment['action'],\n",
    "                    'removal_reasons': quality_assessment['reasons'],\n",
    "                    'confidence': quality_assessment['confidence']\n",
    "                }\n",
    "                \n",
    "                result.update(characteristics)\n",
    "                result.update({f'normalized_{k}': v for k, v in quality_assessment['normalized_scores'].items()})\n",
    "                \n",
    "                results.append(result)\n",
    "                processed_count += 1\n",
    "                \n",
    "                if processed_count % 1000 == 0:\n",
    "                    progress = processed_count / total_images * 100 if total_images > 0 else 0\n",
    "                    print(f\"    Progress: {processed_count:,}/{total_images:,} ({progress:.1f}%)\")\n",
    "                    gc.collect()\n",
    "    \n",
    "    print(f\"  Analysis completed: {processed_count:,} images processed\")\n",
    "    if error_count > 0:\n",
    "        print(f\"  Images with errors: {error_count}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run quality issue identification\n",
    "if valid_datasets and dataset_profiles:\n",
    "    print(\"Starting quality issue identification with 3-component scoring\")\n",
    "    print(\"NEW FORMULA: CQ = 0.25 × BasicQuality + 0.55 × MedicalQuality + 0.20 × TechnicalQuality\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"BasicQuality: brightness, contrast, sharpness, entropy\")\n",
    "    print(\"MedicalQuality: illumination uniformity, vessel visibility, optic disc visibility\")\n",
    "    print(\"TechnicalQuality: extreme pixels, motion blur, color balance\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for dataset_name, profile in dataset_profiles.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        dataset_path = valid_datasets[dataset_name]\n",
    "        results = identify_quality_issues(identifier, dataset_path, dataset_name, profile)\n",
    "        all_results.extend(results)\n",
    "        \n",
    "        flagged = [r for r in results if r['recommended_action'] == 'REMOVE']\n",
    "        flagged_count = len(flagged)\n",
    "        total_count = len(results)\n",
    "        removal_rate = flagged_count / total_count * 100 if total_count > 0 else 0\n",
    "        \n",
    "        print(f\"  Results for {dataset_name}:\")\n",
    "        print(f\"     Total analyzed: {total_count:,}\")\n",
    "        print(f\"     Flagged for removal: {flagged_count:,} ({removal_rate:.1f}%)\")\n",
    "        \n",
    "        # Show average component scores\n",
    "        if results:\n",
    "            avg_basic = np.mean([r['basic_quality_score'] for r in results])\n",
    "            avg_medical = np.mean([r['medical_quality_score'] for r in results])\n",
    "            avg_technical = np.mean([r['technical_quality_score'] for r in results])\n",
    "            avg_overall = np.mean([r['overall_quality_score'] for r in results])\n",
    "            \n",
    "            print(f\"     Average quality scores:\")\n",
    "            print(f\"       Basic (25%): {avg_basic:.3f}\")\n",
    "            print(f\"       Medical (55%): {avg_medical:.3f}\")\n",
    "            print(f\"       Technical (20%): {avg_technical:.3f}\")\n",
    "            print(f\"       Overall: {avg_overall:.3f}\")\n",
    "        \n",
    "        dr_stats = defaultdict(lambda: {'total': 0, 'flagged': 0})\n",
    "        for result in results:\n",
    "            dr_class = result['dr_severity']\n",
    "            dr_stats[dr_class]['total'] += 1\n",
    "            if result['recommended_action'] == 'REMOVE':\n",
    "                dr_stats[dr_class]['flagged'] += 1\n",
    "        \n",
    "        print(f\"     DR class breakdown:\")\n",
    "        dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "        for dr_class in sorted(dr_stats.keys()):\n",
    "            stats = dr_stats[dr_class]\n",
    "            class_removal_rate = stats['flagged'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "            dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "            print(f\"       {dr_name}: {stats['flagged']:,}/{stats['total']:,} ({class_removal_rate:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUALITY IDENTIFICATION COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total images analyzed: {len(all_results):,}\")\n",
    "    \n",
    "    # Overall statistics with component breakdown\n",
    "    if all_results:\n",
    "        total_flagged = len([r for r in all_results if r['recommended_action'] == 'REMOVE'])\n",
    "        overall_removal_rate = total_flagged / len(all_results) * 100\n",
    "        \n",
    "        avg_basic_all = np.mean([r['basic_quality_score'] for r in all_results])\n",
    "        avg_medical_all = np.mean([r['medical_quality_score'] for r in all_results])\n",
    "        avg_technical_all = np.mean([r['technical_quality_score'] for r in all_results])\n",
    "        avg_overall_all = np.mean([r['overall_quality_score'] for r in all_results])\n",
    "        \n",
    "        print(f\"Overall removal rate: {overall_removal_rate:.1f}%\")\n",
    "        print(f\"Average quality scores across all datasets:\")\n",
    "        print(f\"  Basic Quality (25%): {avg_basic_all:.3f}\")\n",
    "        print(f\"  Medical Quality (55%): {avg_medical_all:.3f}\")\n",
    "        print(f\"  Technical Quality (20%): {avg_technical_all:.3f}\")\n",
    "        print(f\"  Overall Quality: {avg_overall_all:.3f}\")\n",
    "        \n",
    "        print(f\"\\n3-component scoring system applied successfully!\")\n",
    "        print(f\"All 14 quality metrics now properly utilized in assessment.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Quality identification skipped - no valid datasets or profiles available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59dd37e-843e-419f-9faa-f75c714c640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create Flagged Samples\n",
    "\n",
    "# Create visual samples of flagged images for manual review\n",
    "def create_flagged_samples(identifier, results, n_samples_per_dataset=20):\n",
    "    print(\"Creating flagged image samples for review\")\n",
    "   \n",
    "    # Group by dataset\n",
    "    by_dataset = {}\n",
    "    for result in results:\n",
    "        dataset = result['dataset']\n",
    "        if dataset not in by_dataset:\n",
    "            by_dataset[dataset] = []\n",
    "        by_dataset[dataset].append(result)\n",
    "   \n",
    "    total_samples_created = 0\n",
    "   \n",
    "    for dataset_name, dataset_results in by_dataset.items():\n",
    "        print(f\"\\n  Processing {dataset_name}\")\n",
    "       \n",
    "        # Get flagged images\n",
    "        flagged = [r for r in dataset_results if r['recommended_action'] == 'REMOVE']\n",
    "       \n",
    "        if not flagged:\n",
    "            print(f\"    No flagged images found\")\n",
    "            continue\n",
    "       \n",
    "        print(f\"    Found {len(flagged)} flagged images\")\n",
    "       \n",
    "        # Sample different types of issues\n",
    "        sample_dir = f'{identifier.output_dir}/flagged_samples/{dataset_name}'\n",
    "        os.makedirs(sample_dir, exist_ok=True)\n",
    "       \n",
    "        # Group by removal reasons\n",
    "        by_reason = {}\n",
    "        for result in flagged:\n",
    "            for reason in result['removal_reasons']:\n",
    "                if reason not in by_reason:\n",
    "                    by_reason[reason] = []\n",
    "                by_reason[reason].append(result)\n",
    "       \n",
    "        print(f\"    Issue types found: {list(by_reason.keys())}\")\n",
    "       \n",
    "        # Sample from each reason category\n",
    "        samples_copied = 0\n",
    "        for reason, reason_results in by_reason.items():\n",
    "            reason_samples = min(5, len(reason_results), n_samples_per_dataset - samples_copied)\n",
    "            if reason_samples <= 0:\n",
    "                continue\n",
    "           \n",
    "            # Sort by confidence and take most confident removals\n",
    "            reason_results.sort(key=lambda x: x['overall_quality_score'])\n",
    "           \n",
    "            for i, result in enumerate(reason_results[:reason_samples]):\n",
    "                try:\n",
    "                    src_path = result['image_path']\n",
    "                    dst_filename = f'{reason}_{i:02d}_{result[\"filename\"]}'\n",
    "                    dst_path = os.path.join(sample_dir, dst_filename)\n",
    "                    shutil.copy2(src_path, dst_path)\n",
    "                    samples_copied += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error copying flagged sample {src_path}: {e}\")\n",
    "       \n",
    "        print(f\"    Created {samples_copied} flagged samples\")\n",
    "        total_samples_created += samples_copied\n",
    "   \n",
    "    print(f\"\\nTotal flagged samples created: {total_samples_created}\")\n",
    "\n",
    "# Run flagged sample creation\n",
    "if 'all_results' in locals() and all_results:\n",
    "    create_flagged_samples(identifier, all_results)\n",
    "else:\n",
    "    print(\"Flagged sample creation skipped - no results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e0b85-fab8-46ef-940b-651da37b3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Generate Comprehensive Report\n",
    "\n",
    "# Generate comprehensive identification report with statistics and recommendations\n",
    "def generate_identification_report(all_results):\n",
    "    print(\"Generating identification report\")\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_images = len(df)\n",
    "    flagged_for_removal = len(df[df['recommended_action'] == 'REMOVE'])\n",
    "    removal_rate = flagged_for_removal / total_images if total_images > 0 else 0\n",
    "    \n",
    "    report = {\n",
    "        'analysis_summary': {\n",
    "            'total_images_analyzed': total_images,\n",
    "            'images_flagged_for_removal': flagged_for_removal,\n",
    "            'overall_removal_rate': removal_rate,\n",
    "            'analysis_date': datetime.now().isoformat()\n",
    "        },\n",
    "        'dataset_breakdown': {},\n",
    "        'removal_reasons_summary': {},\n",
    "        'quality_score_statistics': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Per-dataset breakdown\n",
    "    for dataset in df['dataset'].unique():\n",
    "        dataset_data = df[df['dataset'] == dataset]\n",
    "        dataset_flagged = len(dataset_data[dataset_data['recommended_action'] == 'REMOVE'])\n",
    "        dataset_total = len(dataset_data)\n",
    "        \n",
    "        # Per-class breakdown\n",
    "        class_breakdown = {}\n",
    "        for dr_class in range(5):\n",
    "            class_data = dataset_data[dataset_data['dr_severity'] == dr_class]\n",
    "            if len(class_data) > 0:\n",
    "                class_flagged = len(class_data[class_data['recommended_action'] == 'REMOVE'])\n",
    "                class_breakdown[dr_class] = {\n",
    "                    'total': len(class_data),\n",
    "                    'flagged': class_flagged,\n",
    "                    'removal_rate': class_flagged / len(class_data)\n",
    "                }\n",
    "        \n",
    "        report['dataset_breakdown'][dataset] = {\n",
    "            'total_images': dataset_total,\n",
    "            'flagged_images': dataset_flagged,\n",
    "            'removal_rate': dataset_flagged / dataset_total if dataset_total > 0 else 0,\n",
    "            'class_breakdown': class_breakdown,\n",
    "            'avg_quality_score': float(dataset_data['overall_quality_score'].mean()),\n",
    "            'quality_score_std': float(dataset_data['overall_quality_score'].std())\n",
    "        }\n",
    "    \n",
    "    # Removal reasons summary\n",
    "    all_reasons = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['recommended_action'] == 'REMOVE':\n",
    "            all_reasons.extend(row['removal_reasons'])\n",
    "    \n",
    "    reason_counts = Counter(all_reasons)\n",
    "    report['removal_reasons_summary'] = dict(reason_counts)\n",
    "    \n",
    "    # Quality score statistics\n",
    "    report['quality_score_statistics'] = {\n",
    "        'mean': float(df['overall_quality_score'].mean()),\n",
    "        'std': float(df['overall_quality_score'].std()),\n",
    "        'min': float(df['overall_quality_score'].min()),\n",
    "        'max': float(df['overall_quality_score'].max()),\n",
    "        'percentiles': {\n",
    "            '10': float(df['overall_quality_score'].quantile(0.1)),\n",
    "            '25': float(df['overall_quality_score'].quantile(0.25)),\n",
    "            '50': float(df['overall_quality_score'].quantile(0.5)),\n",
    "            '75': float(df['overall_quality_score'].quantile(0.75)),\n",
    "            '90': float(df['overall_quality_score'].quantile(0.9))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Generate recommendations\n",
    "    if removal_rate > 0.4:\n",
    "        report['recommendations'].append(\"HIGH removal rate detected. Consider relaxing quality thresholds.\")\n",
    "    \n",
    "    if removal_rate < 0.05:\n",
    "        report['recommendations'].append(\"LOW removal rate detected. Consider tightening quality thresholds.\")\n",
    "    \n",
    "    for dataset, stats in report['dataset_breakdown'].items():\n",
    "        if stats['removal_rate'] > 0.5:\n",
    "            report['recommendations'].append(f\"Very high removal rate for {dataset}. Review dataset-specific thresholds.\")\n",
    "        \n",
    "        # Check for class imbalance in removal\n",
    "        class_rates = [info['removal_rate'] for info in stats['class_breakdown'].values()]\n",
    "        if class_rates and max(class_rates) - min(class_rates) > 0.3:\n",
    "            report['recommendations'].append(f\"Uneven removal rates across DR classes in {dataset}. Consider class-specific adjustments.\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate comprehensive report\n",
    "if 'all_results' in locals() and all_results:\n",
    "    report = generate_identification_report(all_results)\n",
    "    print(\"Report generated successfully\")\n",
    "else:\n",
    "    print(\"Report generation skipped - no results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19275b9a-c565-4fe8-babb-ebbd65210d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save All Results\n",
    "\n",
    "# Save all identification results to files\n",
    "def save_identification_results(identifier, results, report):\n",
    "    print(\"Saving identification results\")\n",
    "   \n",
    "    # Save detailed results\n",
    "    df = pd.DataFrame(results)\n",
    "    results_file = f'{identifier.output_dir}/quality_identification_results.csv'\n",
    "    df.to_csv(results_file, index=False)\n",
    "    print(f\"  Detailed results saved: {results_file}\")\n",
    "   \n",
    "    # Save dataset profiles\n",
    "    profiles_file = f'{identifier.output_dir}/dataset_profiles.json'\n",
    "    with open(profiles_file, 'w') as f:\n",
    "        json.dump(identifier.dataset_profiles, f, indent=2)\n",
    "    print(f\"  Dataset profiles saved: {profiles_file}\")\n",
    "   \n",
    "    # Save identification report\n",
    "    report_file = f'{identifier.output_dir}/identification_report.json'\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    print(f\"  Analysis report saved: {report_file}\")\n",
    "   \n",
    "    # Create summary CSV for easy review\n",
    "    summary_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['recommended_action'] == 'REMOVE':\n",
    "            summary_data.append({\n",
    "                'dataset': row['dataset'],\n",
    "                'filename': row['filename'],\n",
    "                'dr_severity': row['dr_severity'],\n",
    "                'quality_score': row['overall_quality_score'],\n",
    "                'confidence': row['confidence'],\n",
    "                'reasons': ', '.join(row['removal_reasons']),\n",
    "                'image_path': row['image_path']\n",
    "            })\n",
    "   \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_file = f'{identifier.output_dir}/flagged_images_summary.csv'\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"  Flagged summary saved: {summary_file}\")\n",
    "   \n",
    "    return results_file, summary_file, profiles_file, report_file\n",
    "\n",
    "# Save all results\n",
    "if 'all_results' in locals() and 'report' in locals() and all_results:\n",
    "    results_file, summary_file, profiles_file, report_file = save_identification_results(identifier, all_results, report)\n",
    "    print(\"\\nAll results saved successfully\")\n",
    "else:\n",
    "    print(\"Results saving skipped - no data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093eb262-b9c2-4028-84f2-1ab625b17f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Final Summary and Visualization\n",
    "\n",
    "# Display comprehensive final summary\n",
    "if 'all_results' in locals() and 'report' in locals() and all_results:\n",
    "    print(\"=\"*80)\n",
    "    print(\"QUALITY IDENTIFICATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "   \n",
    "    # Overall statistics\n",
    "    total_images = len(all_results)\n",
    "    flagged_images = len([r for r in all_results if r['recommended_action'] == 'REMOVE'])\n",
    "    removal_percentage = flagged_images / total_images * 100 if total_images > 0 else 0\n",
    "   \n",
    "    print(f\"\\nOVERALL STATISTICS:\")\n",
    "    print(f\"   Total images analyzed: {total_images:,}\")\n",
    "    print(f\"   Images flagged for removal: {flagged_images:,}\")\n",
    "    print(f\"   Overall removal rate: {removal_percentage:.1f}%\")\n",
    "   \n",
    "    # Per-dataset breakdown\n",
    "    print(f\"\\nPER-DATASET BREAKDOWN:\")\n",
    "    dataset_stats = defaultdict(lambda: {'total': 0, 'flagged': 0})\n",
    "   \n",
    "    for result in all_results:\n",
    "        dataset = result['dataset']\n",
    "        dataset_stats[dataset]['total'] += 1\n",
    "        if result['recommended_action'] == 'REMOVE':\n",
    "            dataset_stats[dataset]['flagged'] += 1\n",
    "   \n",
    "    for dataset, stats in dataset_stats.items():\n",
    "        removal_rate = stats['flagged'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "        print(f\"   {dataset}: {stats['flagged']:,}/{stats['total']:,} ({removal_rate:.1f}%)\")\n",
    "   \n",
    "    # Per-DR class breakdown\n",
    "    print(f\"\\nPER-DR CLASS BREAKDOWN:\")\n",
    "    dr_stats = defaultdict(lambda: {'total': 0, 'flagged': 0})\n",
    "   \n",
    "    for result in all_results:\n",
    "        dr_class = result['dr_severity']\n",
    "        dr_stats[dr_class]['total'] += 1\n",
    "        if result['recommended_action'] == 'REMOVE':\n",
    "            dr_stats[dr_class]['flagged'] += 1\n",
    "   \n",
    "    dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "    for dr_class in sorted(dr_stats.keys()):\n",
    "        stats = dr_stats[dr_class]\n",
    "        removal_rate = stats['flagged'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "        dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "        print(f\"   {dr_name}: {stats['flagged']:,}/{stats['total']:,} ({removal_rate:.1f}%)\")\n",
    "   \n",
    "    # Top removal reasons\n",
    "    print(f\"\\nTOP REMOVAL REASONS:\")\n",
    "    if 'removal_reasons_summary' in report:\n",
    "        sorted_reasons = sorted(report['removal_reasons_summary'].items(), key=lambda x: x[1], reverse=True)\n",
    "        for reason, count in sorted_reasons[:10]:  # Top 10 reasons\n",
    "            percentage = count / flagged_images * 100 if flagged_images > 0 else 0\n",
    "            print(f\"   {reason}: {count:,} ({percentage:.1f}% of flagged images)\")\n",
    "   \n",
    "    # Recommendations\n",
    "    if 'recommendations' in report and report['recommendations']:\n",
    "        print(f\"\\nRECOMMENDATIONS:\")\n",
    "        for i, recommendation in enumerate(report['recommendations'], 1):\n",
    "            print(f\"   {i}. {recommendation}\")\n",
    "   \n",
    "    # File locations\n",
    "    print(f\"\\nRESULTS SAVED TO:\")\n",
    "    print(f\"   Flagged images summary: {OUTPUT_DIR}/flagged_images_summary.csv\")\n",
    "    print(f\"   Detailed results: {OUTPUT_DIR}/quality_identification_results.csv\")\n",
    "    print(f\"   Analysis report: {OUTPUT_DIR}/identification_report.json\")\n",
    "    print(f\"   Sample images: {OUTPUT_DIR}/sample_images/\")\n",
    "    print(f\"   Flagged samples: {OUTPUT_DIR}/flagged_samples/\")\n",
    "   \n",
    "    print(f\"\\nNEXT STEPS:\")\n",
    "    print(f\"   1. Review flagged samples in: {OUTPUT_DIR}/flagged_samples/\")\n",
    "    print(f\"   2. Check the summary CSV for a list of all flagged images\")\n",
    "    print(f\"   3. Adjust thresholds if needed (modify dataset profiles)\")\n",
    "    print(f\"   4. Use the detailed results for actual image removal when ready\")\n",
    "   \n",
    "    print(f\"\\nQuality identification process completed successfully\")\n",
    "else:\n",
    "    print(\"No results to summarize - please run all previous cells first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe342e61-aff3-42f7-afca-69cf8f233494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Quick Visualization\n",
    "\n",
    "# Create basic visualizations for quality analysis results\n",
    "if 'all_results' in locals() and all_results:\n",
    "    print(\"Creating basic visualizations\")\n",
    "    \n",
    "    # Set up matplotlib\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('DR Dataset Quality Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # 1. Removal rates by dataset\n",
    "    ax1 = axes[0, 0]\n",
    "    dataset_removal_rates = []\n",
    "    dataset_names = []\n",
    "    \n",
    "    for dataset in df['dataset'].unique():\n",
    "        dataset_data = df[df['dataset'] == dataset]\n",
    "        removal_rate = len(dataset_data[dataset_data['recommended_action'] == 'REMOVE']) / len(dataset_data) * 100\n",
    "        dataset_removal_rates.append(removal_rate)\n",
    "        dataset_names.append(dataset)\n",
    "    \n",
    "    bars1 = ax1.bar(range(len(dataset_names)), dataset_removal_rates, color='lightcoral', alpha=0.7)\n",
    "    ax1.set_xlabel('Dataset')\n",
    "    ax1.set_ylabel('Removal Rate (%)')\n",
    "    ax1.set_title('Removal Rates by Dataset')\n",
    "    ax1.set_xticks(range(len(dataset_names)))\n",
    "    ax1.set_xticklabels(dataset_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, rate in zip(bars1, dataset_removal_rates):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Removal rates by DR class\n",
    "    ax2 = axes[0, 1]\n",
    "    dr_removal_rates = []\n",
    "    dr_labels = []\n",
    "    dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "    \n",
    "    for dr_class in sorted(df['dr_severity'].unique()):\n",
    "        dr_data = df[df['dr_severity'] == dr_class]\n",
    "        removal_rate = len(dr_data[dr_data['recommended_action'] == 'REMOVE']) / len(dr_data) * 100\n",
    "        dr_removal_rates.append(removal_rate)\n",
    "        dr_label = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "        dr_labels.append(dr_label)\n",
    "    \n",
    "    bars2 = ax2.bar(range(len(dr_labels)), dr_removal_rates, color='lightblue', alpha=0.7)\n",
    "    ax2.set_xlabel('DR Severity')\n",
    "    ax2.set_ylabel('Removal Rate (%)')\n",
    "    ax2.set_title('Removal Rates by DR Severity')\n",
    "    ax2.set_xticks(range(len(dr_labels)))\n",
    "    ax2.set_xticklabels(dr_labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, rate in zip(bars2, dr_removal_rates):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Quality score distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    keep_scores = df[df['recommended_action'] == 'KEEP']['overall_quality_score']\n",
    "    remove_scores = df[df['recommended_action'] == 'REMOVE']['overall_quality_score']\n",
    "    \n",
    "    ax3.hist(keep_scores, bins=30, alpha=0.7, label='Keep', color='lightgreen', density=True)\n",
    "    ax3.hist(remove_scores, bins=30, alpha=0.7, label='Remove', color='lightcoral', density=True)\n",
    "    ax3.set_xlabel('Quality Score')\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.set_title('Quality Score Distribution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Top removal reasons\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'removal_reasons_summary' in report:\n",
    "        reasons = list(report['removal_reasons_summary'].keys())[:10]  # Top 10\n",
    "        counts = [report['removal_reasons_summary'][reason] for reason in reasons]\n",
    "        \n",
    "        bars4 = ax4.barh(range(len(reasons)), counts, color='orange', alpha=0.7)\n",
    "        ax4.set_ylabel('Removal Reason')\n",
    "        ax4.set_xlabel('Count')\n",
    "        ax4.set_title('Top Removal Reasons')\n",
    "        ax4.set_yticks(range(len(reasons)))\n",
    "        ax4.set_yticklabels([reason.replace('_', ' ').title() for reason in reasons])\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars4, counts):\n",
    "            width = bar.get_width()\n",
    "            ax4.text(width + max(counts)*0.01, bar.get_y() + bar.get_height()/2.,\n",
    "                    f'{count}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_file = f'{OUTPUT_DIR}/analysis_summary_plots.png'\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Visualization saved to: {plot_file}\")\n",
    "\n",
    "else:\n",
    "    print(\"Visualization creation skipped - no results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee55f3e-69d7-458f-8a35-ac29d1439a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Final Checkpoint - Verification\n",
    "\n",
    "# Final verification that all components completed successfully\n",
    "print(\"FINAL VERIFICATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if all major components completed\n",
    "checks = {\n",
    "    'Valid datasets found': 'valid_datasets' in locals() and bool(valid_datasets),\n",
    "    'Dataset profiles created': 'dataset_profiles' in locals() and bool(dataset_profiles),\n",
    "    'Quality analysis completed': 'all_results' in locals() and bool(all_results),\n",
    "    'Report generated': 'report' in locals() and bool(report),\n",
    "    'Results saved': os.path.exists(f'{OUTPUT_DIR}/flagged_images_summary.csv'),\n",
    "    'Sample images created': os.path.exists(f'{OUTPUT_DIR}/sample_images'),\n",
    "    'Flagged samples created': os.path.exists(f'{OUTPUT_DIR}/flagged_samples')\n",
    "}\n",
    "\n",
    "all_good = True\n",
    "for check_name, check_result in checks.items():\n",
    "    status = \"PASS\" if check_result else \"FAIL\"\n",
    "    print(f\"{status}: {check_name}\")\n",
    "    if not check_result:\n",
    "        all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(f\"\\nSUCCESS: All components completed successfully\")\n",
    "    print(f\"Check the '{OUTPUT_DIR}' directory for all results\")\n",
    "   \n",
    "    if 'all_results' in locals():\n",
    "        total = len(all_results)\n",
    "        flagged = len([r for r in all_results if r['recommended_action'] == 'REMOVE'])\n",
    "        print(f\"Final count: {flagged:,} of {total:,} images flagged for removal ({flagged/total*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Some components did not complete successfully\")\n",
    "    print(f\"Please review the error messages above and re-run the failed sections\")\n",
    "\n",
    "print(f\"\\nIMPORTANT: This analysis only IDENTIFIED potential issues\")\n",
    "print(f\"NO IMAGES WERE ACTUALLY REMOVED from your datasets\")\n",
    "print(f\"Review the flagged samples before deciding on actual removal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Check what sample images were saved\n",
    "import os\n",
    "sample_base_dir = f'{identifier.output_dir}/sample_images'\n",
    "print(\"Sample images saved:\")\n",
    "for dataset_name in os.listdir(sample_base_dir):\n",
    "    dataset_sample_dir = os.path.join(sample_base_dir, dataset_name)\n",
    "    if os.path.isdir(dataset_sample_dir):\n",
    "        num_samples = len([f for f in os.listdir(dataset_sample_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"   {dataset_name}: {num_samples} sample images in {dataset_sample_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875afcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Problematic Images Review\n",
    "\n",
    "# Find and organize problematic images for manual review\n",
    "def find_and_review_problematic_images(identifier, dataset_profiles,\n",
    "                                     review_dir='problematic_images_review_all'):\n",
    "    \n",
    "    # Create review directory\n",
    "    os.makedirs(review_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Finding problematic images based on quality thresholds\")\n",
    "    \n",
    "    for dataset_name, profile in dataset_profiles.items():\n",
    "        print(f\"\\nProcessing {dataset_name}\")\n",
    "        \n",
    "        # Get adaptive thresholds for this dataset\n",
    "        thresholds = profile['adaptive_thresholds']\n",
    "        \n",
    "        # Create subdirectories for this dataset\n",
    "        dataset_review_dir = os.path.join(review_dir, dataset_name)\n",
    "        os.makedirs(dataset_review_dir, exist_ok=True)\n",
    "        \n",
    "        # Analyze all images in the dataset\n",
    "        dataset_path = valid_datasets[dataset_name]\n",
    "        all_images = []\n",
    "        \n",
    "        # Collect all images from ALL subdirectories (all DR severity levels)\n",
    "        for root, dirs, files in os.walk(dataset_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    all_images.append(os.path.join(root, file))\n",
    "        \n",
    "        print(f\"   Analyzing {len(all_images)} images\")\n",
    "        \n",
    "        problematic_by_severity = {0: [], 1: [], 2: [], 3: [], 4: []}\n",
    "        \n",
    "        # Analyze each image\n",
    "        for i, img_path in enumerate(all_images):\n",
    "            if i % 500 == 0 and i > 0:\n",
    "                print(f\"      Progress: {i}/{len(all_images)} ({i/len(all_images)*100:.1f}%)\")\n",
    "            \n",
    "            try:\n",
    "                # Get image characteristics\n",
    "                char = identifier.analyze_single_image(img_path)\n",
    "                if not char:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate quality score (same logic as in profiling)\n",
    "                brightness = float(char['brightness']) if char['brightness'] is not None else 127.5\n",
    "                contrast = float(char['contrast']) if char['contrast'] is not None else 50.0\n",
    "                sharpness = float(char['sharpness']) if char['sharpness'] is not None else 500.0\n",
    "                entropy = float(char['entropy']) if char['entropy'] is not None else 4.0\n",
    "                illumination_uniformity = float(char['illumination_uniformity']) if char['illumination_uniformity'] is not None else 0.5\n",
    "                vessel_visibility = float(char['vessel_visibility']) if char['vessel_visibility'] is not None else 0.1\n",
    "                optic_disc_visibility = float(char['optic_disc_visibility']) if char['optic_disc_visibility'] is not None else 0.1\n",
    "                \n",
    "                # Normalize metrics\n",
    "                brightness_norm = min(1.0, max(0.0, brightness / 255.0))\n",
    "                contrast_norm = min(1.0, max(0.0, contrast / 100.0))\n",
    "                sharpness_norm = min(1.0, max(0.0, sharpness / 1000.0))\n",
    "                entropy_norm = min(1.0, max(0.0, entropy / 8.0))\n",
    "                \n",
    "                basic_quality = np.mean([brightness_norm, contrast_norm, sharpness_norm, entropy_norm])\n",
    "                medical_quality = np.mean([\n",
    "                    illumination_uniformity,\n",
    "                    min(1.0, vessel_visibility * 10),\n",
    "                    min(1.0, optic_disc_visibility * 10)\n",
    "                ])\n",
    "                \n",
    "                combined_quality = 0.3 * basic_quality + 0.7 * medical_quality\n",
    "                \n",
    "                # Determine DR severity from path or filename\n",
    "                dr_severity = get_dr_severity_from_path(img_path)\n",
    "                \n",
    "                # Check if image is below threshold for its DR severity\n",
    "                if dr_severity in thresholds and combined_quality < thresholds[dr_severity]:\n",
    "                    problematic_by_severity[dr_severity].append({\n",
    "                        'path': img_path,\n",
    "                        'quality_score': combined_quality,\n",
    "                        'threshold': thresholds[dr_severity],\n",
    "                        'characteristics': char\n",
    "                    })\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"      Error analyzing {img_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save problematic images by severity\n",
    "        total_problematic = 0\n",
    "        for severity, images in problematic_by_severity.items():\n",
    "            if images:\n",
    "                severity_dir = os.path.join(dataset_review_dir, f'DR_severity_{severity}')\n",
    "                os.makedirs(severity_dir, exist_ok=True)\n",
    "                \n",
    "                # Sort by quality score (worst first)\n",
    "                images.sort(key=lambda x: x['quality_score'])\n",
    "                \n",
    "                print(f\"   DR Severity {severity}: {len(images)} problematic images\")\n",
    "                \n",
    "                # Copy worst 50 images for manual review\n",
    "                for i, img_info in enumerate(images[:50]):\n",
    "                    try:\n",
    "                        src_path = img_info['path']\n",
    "                        dst_name = f\"quality_{img_info['quality_score']:.3f}_{os.path.basename(src_path)}\"\n",
    "                        dst_path = os.path.join(severity_dir, dst_name)\n",
    "                        shutil.copy2(src_path, dst_path)\n",
    "                        \n",
    "                        # Save characteristics as text file\n",
    "                        txt_path = dst_path.replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt')\n",
    "                        with open(txt_path, 'w') as f:\n",
    "                            f.write(f\"Quality Score: {img_info['quality_score']:.3f}\\n\")\n",
    "                            f.write(f\"Threshold: {img_info['threshold']:.3f}\\n\")\n",
    "                            f.write(f\"Original Path: {src_path}\\n\\n\")\n",
    "                            f.write(\"Characteristics:\\n\")\n",
    "                            for key, value in img_info['characteristics'].items():\n",
    "                                f.write(f\"  {key}: {value}\\n\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"      Error copying {src_path}: {e}\")\n",
    "                \n",
    "                total_problematic += len(images)\n",
    "        \n",
    "        print(f\"   Found {total_problematic} total problematic images in {dataset_name}\")\n",
    "        print(f\"   Review images saved to: {dataset_review_dir}\")\n",
    "    \n",
    "    print(f\"\\nReview complete. Check the '{review_dir}' directory\")\n",
    "\n",
    "# Extract DR severity from image path or filename\n",
    "def get_dr_severity_from_path(img_path):\n",
    "    import re\n",
    "    path_lower = img_path.lower()\n",
    "    \n",
    "    # First, check for direct folder patterns like /0/, /1/, /2/, /3/, /4/\n",
    "    folder_match = re.search(r'[/\\\\]([0-4])[/\\\\]', img_path)\n",
    "    if folder_match:\n",
    "        return int(folder_match.group(1))\n",
    "    \n",
    "    # Check for keyword-based patterns\n",
    "    if 'no_dr' in path_lower or 'grade_0' in path_lower or '_0_' in path_lower:\n",
    "        return 0\n",
    "    elif 'mild' in path_lower or 'grade_1' in path_lower or '_1_' in path_lower:\n",
    "        return 1\n",
    "    elif 'moderate' in path_lower or 'grade_2' in path_lower or '_2_' in path_lower:\n",
    "        return 2\n",
    "    elif 'severe' in path_lower or 'grade_3' in path_lower or '_3_' in path_lower:\n",
    "        return 3\n",
    "    elif 'proliferative' in path_lower or 'grade_4' in path_lower or '_4_' in path_lower:\n",
    "        return 4\n",
    "    \n",
    "    # Look for patterns like \"grade_2\", \"severity_1\", etc.\n",
    "    grade_match = re.search(r'grade[_\\-]?([0-4])', path_lower)\n",
    "    if grade_match:\n",
    "        return int(grade_match.group(1))\n",
    "    \n",
    "    severity_match = re.search(r'severity[_\\-]?([0-4])', path_lower)\n",
    "    if severity_match:\n",
    "        return int(severity_match.group(1))\n",
    "    \n",
    "    # Look for numeric patterns in filename\n",
    "    filename_match = re.search(r'[_\\-]([0-4])[_\\-\\.]', path_lower)\n",
    "    if filename_match:\n",
    "        return int(filename_match.group(1))\n",
    "    \n",
    "    # Default to 0 if can't determine\n",
    "    print(f\"   Could not determine DR severity for: {img_path}\")\n",
    "    return 0\n",
    "\n",
    "# Create an HTML file for easy image review\n",
    "def create_review_html(review_dir='problematic_images_review_all'):\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Problematic Images Review</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "            .dataset { margin-bottom: 30px; border: 1px solid #ccc; padding: 15px; }\n",
    "            .severity { margin-bottom: 20px; }\n",
    "            .image-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(200px, 1fr)); gap: 10px; }\n",
    "            .image-item { border: 1px solid #ddd; padding: 10px; text-align: center; }\n",
    "            .image-item img { max-width: 100%; height: 150px; object-fit: cover; }\n",
    "            .quality-score { font-weight: bold; color: red; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Problematic Images Review</h1>\n",
    "    \"\"\"\n",
    "    \n",
    "    for dataset_name in os.listdir(review_dir):\n",
    "        dataset_path = os.path.join(review_dir, dataset_name)\n",
    "        if not os.path.isdir(dataset_path):\n",
    "            continue\n",
    "        \n",
    "        html_content += f'<div class=\"dataset\"><h2>{dataset_name}</h2>'\n",
    "        \n",
    "        for severity_dir in sorted(os.listdir(dataset_path)):\n",
    "            severity_path = os.path.join(dataset_path, severity_dir)\n",
    "            if not os.path.isdir(severity_path):\n",
    "                continue\n",
    "            \n",
    "            html_content += f'<div class=\"severity\"><h3>{severity_dir}</h3><div class=\"image-grid\">'\n",
    "            \n",
    "            images = [f for f in os.listdir(severity_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            for img_file in sorted(images)[:20]:  # Show first 20\n",
    "                quality_score = img_file.split('_')[1] if '_' in img_file else 'unknown'\n",
    "                img_rel_path = os.path.join(dataset_name, severity_dir, img_file).replace('\\\\', '/')\n",
    "                \n",
    "                html_content += f'''\n",
    "                <div class=\"image-item\">\n",
    "                    <img src=\"{img_rel_path}\" alt=\"{img_file}\">\n",
    "                    <div class=\"quality-score\">Quality: {quality_score}</div>\n",
    "                    <div>{img_file}</div>\n",
    "                </div>\n",
    "                '''\n",
    "            \n",
    "            html_content += '</div></div>'\n",
    "        \n",
    "        html_content += '</div>'\n",
    "    \n",
    "    html_content += '</body></html>'\n",
    "    \n",
    "    html_path = os.path.join(review_dir, 'review.html')\n",
    "    with open(html_path, 'w') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"HTML review file created: {html_path}\")\n",
    "    print(\"   Open this file in your web browser to easily review problematic images\")\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Starting problematic image identification\")\n",
    "find_and_review_problematic_images(identifier, dataset_profiles)\n",
    "\n",
    "# Create HTML review file\n",
    "create_review_html()\n",
    "\n",
    "print(\"\\nReview setup complete\")\n",
    "print(\"\\nTo review the images:\")\n",
    "print(\"1. Check the 'problematic_images_review_all' directory\")\n",
    "print(\"2. Open 'problematic_images_review_all/review.html' in your web browser\")\n",
    "print(\"3. Each image has a quality score and characteristics file (.txt)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Manual Threshold Adjustment (Optional)\n",
    "\n",
    "print(\"MANUAL THRESHOLD ADJUSTMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Use this cell to fine-tune removal thresholds for each dataset\")\n",
    "print(\"Lower thresholds = more images flagged for removal\")\n",
    "print(\"Higher thresholds = fewer images flagged for removal\")\n",
    "print(\"Current thresholds are based on your dataset analysis\")\n",
    "\n",
    "# Check if dataset_profiles exists\n",
    "if 'dataset_profiles' not in locals() or not dataset_profiles:\n",
    "    print(\"\\nERROR: Dataset profiles not found\")\n",
    "    print(\"Please run the previous cells first to create dataset profiles.\")\n",
    "    print(\"Required steps:\")\n",
    "    print(\"1. Run dataset validation (Cell 3)\")\n",
    "    print(\"2. Run dataset characterization (Cell 6)\")\n",
    "    print(\"3. Then come back to this cell\")\n",
    "else:\n",
    "    # Display current thresholds\n",
    "    print(\"\\nCURRENT THRESHOLDS:\")\n",
    "    for dataset_name, profile in dataset_profiles.items():\n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        for dr_class, threshold in profile['adaptive_thresholds'].items():\n",
    "            dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "            dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "            print(f\"  {dr_name} (Class {dr_class}): {threshold:.3f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MANUAL ADJUSTMENT SECTION\")\n",
    "    print(\"Edit the values below to adjust thresholds\")\n",
    "    print(\"Set to None to keep current value\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Manual threshold override dictionary\n",
    "    # Users can edit these values\n",
    "    manual_thresholds = {\n",
    "        'APTOS2019': {\n",
    "            0: None,  # No DR - set to None to keep current, or set value like 0.250\n",
    "            1: None,  # Mild DR\n",
    "            2: None,  # Moderate DR  \n",
    "            3: None,  # Severe DR\n",
    "            4: None   # Proliferative DR\n",
    "        },\n",
    "        'Diabetic_Retinopathy_V03': {\n",
    "            0: None,\n",
    "            1: None,\n",
    "            2: None,\n",
    "            3: None,\n",
    "            4: None\n",
    "        },\n",
    "        'IDRiD': {\n",
    "            0: None,\n",
    "            1: None,\n",
    "            2: None,\n",
    "            3: None,\n",
    "            4: None\n",
    "        },\n",
    "        'Messidor2': {\n",
    "            0: None,\n",
    "            1: None,\n",
    "            2: None,\n",
    "            3: None,\n",
    "            4: None\n",
    "        },\n",
    "        'SUSTech_SYSU': {\n",
    "            0: None,\n",
    "            1: None,\n",
    "            2: None,\n",
    "            3: None,\n",
    "            4: None\n",
    "        },\n",
    "        'DeepDRiD': {\n",
    "            0: None,\n",
    "            1: None,\n",
    "            2: None,\n",
    "            3: None,\n",
    "            4: None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Apply manual overrides\n",
    "    updated_profiles = {}\n",
    "    changes_made = False\n",
    "\n",
    "    for dataset_name, profile in dataset_profiles.items():\n",
    "        updated_profiles[dataset_name] = profile.copy()\n",
    "        updated_profiles[dataset_name]['adaptive_thresholds'] = profile['adaptive_thresholds'].copy()\n",
    "        \n",
    "        if dataset_name in manual_thresholds:\n",
    "            for dr_class, manual_threshold in manual_thresholds[dataset_name].items():\n",
    "                if manual_threshold is not None:\n",
    "                    old_threshold = profile['adaptive_thresholds'][dr_class]\n",
    "                    updated_profiles[dataset_name]['adaptive_thresholds'][dr_class] = manual_threshold\n",
    "                    changes_made = True\n",
    "                    dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "                    dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "                    print(f\"Updated {dataset_name} - {dr_name}: {old_threshold:.3f} → {manual_threshold:.3f}\")\n",
    "\n",
    "    if changes_made:\n",
    "        print(f\"\\nThreshold adjustments applied\")\n",
    "        print(\"Updated profiles will be used for final dataset creation.\")\n",
    "        # Update the global profiles\n",
    "        dataset_profiles = updated_profiles\n",
    "    else:\n",
    "        print(f\"\\nNo manual adjustments made. Using original thresholds\")\n",
    "\n",
    "    print(f\"\\nFINAL THRESHOLDS AFTER ADJUSTMENT:\")\n",
    "    for dataset_name, profile in dataset_profiles.items():\n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        for dr_class, threshold in profile['adaptive_thresholds'].items():\n",
    "            dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "            dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "            print(f\"  {dr_name} (Class {dr_class}): {threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97871da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Dataset Cleaning with 3-Component Quality Scoring\n",
    "\n",
    "def create_cleaned_dataset(source_datasets, profiles, copy_output_dir='DREAM_dataset_cleaned'):\n",
    "    \"\"\"\n",
    "    Create cleaned dataset using 3-component quality scoring system.\n",
    "    \n",
    "    Args:\n",
    "        source_datasets: Dictionary mapping dataset names to source paths\n",
    "        profiles: Dictionary containing dataset quality profiles and thresholds\n",
    "        copy_output_dir: Output directory for cleaned dataset\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (statistics_dict, processing_time)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Creating cleaned dataset: {copy_output_dir}\")\n",
    "    print(\"Quality scoring: CQ = 0.25×Basic + 0.55×Medical + 0.20×Technical\")\n",
    "    \n",
    "    os.makedirs(copy_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize statistics tracking\n",
    "    copy_stats = {\n",
    "        'total_processed': 0, 'total_kept': 0, 'total_removed': 0,\n",
    "        'by_dataset': {}, 'by_dr_class': {i: {'kept': 0, 'removed': 0} for i in range(5)}\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset_name, dataset_path in source_datasets.items():\n",
    "        print(f\"\\nProcessing {dataset_name}\")\n",
    "        \n",
    "        if dataset_name not in profiles:\n",
    "            print(f\"  Profile not found for {dataset_name}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        profile = profiles[dataset_name]\n",
    "        thresholds = profile['adaptive_thresholds']\n",
    "        \n",
    "        # Create dataset output directory\n",
    "        copy_dataset_dir = os.path.join(copy_output_dir, dataset_name)\n",
    "        os.makedirs(copy_dataset_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize dataset-level statistics\n",
    "        copy_dataset_stats = {\n",
    "            'total_processed': 0, 'total_kept': 0, 'total_removed': 0,\n",
    "            'by_dr_class': {i: {'kept': 0, 'removed': 0} for i in range(5)}\n",
    "        }\n",
    "        \n",
    "        # Process each DR severity class\n",
    "        for dr_class in range(5):\n",
    "            dr_class_path = os.path.join(dataset_path, str(dr_class))\n",
    "            \n",
    "            if not os.path.exists(dr_class_path):\n",
    "                continue\n",
    "            \n",
    "            # Create output directory for this DR class\n",
    "            copy_dr_dir = os.path.join(copy_dataset_dir, str(dr_class))\n",
    "            os.makedirs(copy_dr_dir, exist_ok=True)\n",
    "            \n",
    "            # Get image files\n",
    "            image_files = [f for f in os.listdir(dr_class_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff', '.bmp'))]\n",
    "            \n",
    "            if not image_files:\n",
    "                continue\n",
    "                \n",
    "            print(f\"  DR Class {dr_class}: Processing {len(image_files)} images\")\n",
    "            \n",
    "            threshold = thresholds.get(dr_class, 0.3)\n",
    "            kept_count = 0\n",
    "            removed_count = 0\n",
    "            \n",
    "            # Process images in this class\n",
    "            for i, image_file in enumerate(image_files):\n",
    "                if i % 500 == 0 and i > 0:\n",
    "                    progress = (i / len(image_files)) * 100\n",
    "                    print(f\"    Progress: {i}/{len(image_files)} ({progress:.1f}%)\")\n",
    "                \n",
    "                try:\n",
    "                    source_image_path = os.path.join(dr_class_path, image_file)\n",
    "                    \n",
    "                    # Analyze image quality\n",
    "                    char = identifier.analyze_single_image(source_image_path)\n",
    "                    if not char:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate 3-component quality score\n",
    "                    combined_quality = calculate_composite_quality(char)\n",
    "                    \n",
    "                    # Apply quality threshold\n",
    "                    if combined_quality >= threshold:\n",
    "                        # Keep image\n",
    "                        copy_dest_path = os.path.join(copy_dr_dir, image_file)\n",
    "                        shutil.copy2(source_image_path, copy_dest_path)\n",
    "                        kept_count += 1\n",
    "                        copy_dataset_stats['by_dr_class'][dr_class]['kept'] += 1\n",
    "                        copy_stats['by_dr_class'][dr_class]['kept'] += 1\n",
    "                    else:\n",
    "                        # Remove image\n",
    "                        removed_count += 1\n",
    "                        copy_dataset_stats['by_dr_class'][dr_class]['removed'] += 1\n",
    "                        copy_stats['by_dr_class'][dr_class]['removed'] += 1\n",
    "                    \n",
    "                    copy_dataset_stats['total_processed'] += 1\n",
    "                    copy_stats['total_processed'] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error processing {image_file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Update statistics\n",
    "            copy_dataset_stats['total_kept'] += kept_count\n",
    "            copy_dataset_stats['total_removed'] += removed_count\n",
    "            \n",
    "            # Report class results\n",
    "            if kept_count + removed_count > 0:\n",
    "                removal_rate = (removed_count / (kept_count + removed_count)) * 100\n",
    "                dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "                dr_name = dr_names[dr_class]\n",
    "                print(f\"    {dr_name}: Kept {kept_count}, Removed {removed_count} ({removal_rate:.1f}% removed)\")\n",
    "        \n",
    "        # Update total statistics\n",
    "        copy_stats['total_kept'] += copy_dataset_stats['total_kept']\n",
    "        copy_stats['total_removed'] += copy_dataset_stats['total_removed']\n",
    "        copy_stats['by_dataset'][dataset_name] = copy_dataset_stats\n",
    "        \n",
    "        # Dataset summary\n",
    "        if copy_dataset_stats['total_processed'] > 0:\n",
    "            dataset_removal_rate = (copy_dataset_stats['total_removed'] / copy_dataset_stats['total_processed']) * 100\n",
    "            print(f\"  {dataset_name} Summary: {copy_dataset_stats['total_processed']:,} processed, \"\n",
    "                  f\"{copy_dataset_stats['total_kept']:,} kept, \"\n",
    "                  f\"{copy_dataset_stats['total_removed']:,} removed ({dataset_removal_rate:.1f}%)\")\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Save statistics\n",
    "    copy_stats_file = os.path.join(copy_output_dir, 'cleaning_statistics.json')\n",
    "    with open(copy_stats_file, 'w') as f:\n",
    "        json.dump(copy_stats, f, indent=2)\n",
    "    \n",
    "    return copy_stats, processing_time\n",
    "\n",
    "def calculate_composite_quality(characteristics):\n",
    "    \"\"\"\n",
    "    Calculate composite quality score using 3-component system.\n",
    "    \n",
    "    Args:\n",
    "        characteristics: Dictionary of image quality characteristics\n",
    "        \n",
    "    Returns:\n",
    "        float: Composite quality score (0-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract and validate metrics\n",
    "    brightness = float(characteristics.get('brightness', 127.5))\n",
    "    contrast = float(characteristics.get('contrast', 50.0))\n",
    "    sharpness = float(characteristics.get('sharpness', 500.0))\n",
    "    entropy = float(characteristics.get('entropy', 4.0))\n",
    "    \n",
    "    illumination_uniformity = float(characteristics.get('illumination_uniformity', 0.5))\n",
    "    vessel_visibility = float(characteristics.get('vessel_visibility', 0.1))\n",
    "    optic_disc_visibility = float(characteristics.get('optic_disc_visibility', 0.1))\n",
    "    \n",
    "    extreme_pixels = float(characteristics.get('extreme_brightness_pixels', 0.1))\n",
    "    motion_blur = float(characteristics.get('motion_blur_score', 20.0))\n",
    "    color_balance = float(characteristics.get('color_balance', 15.0))\n",
    "    \n",
    "    # Normalize Basic Quality metrics\n",
    "    brightness_norm = min(1.0, max(0.0, brightness / 255.0))\n",
    "    contrast_norm = min(1.0, max(0.0, contrast / 100.0))\n",
    "    sharpness_norm = min(1.0, max(0.0, sharpness / 1000.0))\n",
    "    entropy_norm = min(1.0, max(0.0, entropy / 8.0))\n",
    "    \n",
    "    basic_quality = np.mean([brightness_norm, contrast_norm, sharpness_norm, entropy_norm])\n",
    "    \n",
    "    # Normalize Medical Quality metrics\n",
    "    medical_quality = np.mean([\n",
    "        min(1.0, max(0.0, illumination_uniformity)),\n",
    "        min(1.0, max(0.0, vessel_visibility * 10)),\n",
    "        min(1.0, max(0.0, optic_disc_visibility * 10))\n",
    "    ])\n",
    "    \n",
    "    # Normalize Technical Quality metrics\n",
    "    extreme_pixels_norm = max(0, min(1, 1 - (extreme_pixels * 2)))  # Fewer is better\n",
    "    motion_blur_norm = min(1.0, max(0.0, motion_blur / 50.0))        # Higher is better\n",
    "    color_balance_norm = max(0, min(1, 1 - (color_balance / 50.0)))  # Lower std is better\n",
    "    \n",
    "    technical_quality = np.mean([extreme_pixels_norm, motion_blur_norm, color_balance_norm])\n",
    "    \n",
    "    # Composite formula: CQ = 0.25×Basic + 0.55×Medical + 0.20×Technical\n",
    "    return 0.25 * basic_quality + 0.55 * medical_quality + 0.20 * technical_quality\n",
    "\n",
    "def generate_cleaning_report(stats, processing_time, output_dir):\n",
    "    \"\"\"\n",
    "    Generate concise cleaning report.\n",
    "    \n",
    "    Args:\n",
    "        stats: Cleaning statistics dictionary\n",
    "        processing_time: Total processing time in seconds\n",
    "        output_dir: Output directory path\n",
    "    \"\"\"\n",
    "    \n",
    "    total_processed = stats['total_processed']\n",
    "    total_kept = stats['total_kept']\n",
    "    total_removed = stats['total_removed']\n",
    "    \n",
    "    if total_processed == 0:\n",
    "        print(\"No images processed\")\n",
    "        return\n",
    "    \n",
    "    overall_removal_rate = (total_removed / total_processed) * 100\n",
    "    \n",
    "    print(f\"\\nCleaning Summary:\")\n",
    "    print(f\"Processing time: {processing_time/60:.1f} minutes\")\n",
    "    print(f\"Total processed: {total_processed:,}\")\n",
    "    print(f\"Images kept: {total_kept:,}\")\n",
    "    print(f\"Images removed: {total_removed:,} ({overall_removal_rate:.1f}%)\")\n",
    "    \n",
    "    # Per-dataset summary\n",
    "    print(f\"\\nDataset breakdown:\")\n",
    "    for dataset_name, dataset_stats in stats['by_dataset'].items():\n",
    "        if dataset_stats['total_processed'] > 0:\n",
    "            removal_rate = (dataset_stats['total_removed'] / dataset_stats['total_processed']) * 100\n",
    "            print(f\"  {dataset_name}: {dataset_stats['total_kept']:,} kept, \"\n",
    "                  f\"{dataset_stats['total_removed']:,} removed ({removal_rate:.1f}%)\")\n",
    "    \n",
    "    # Per-DR class summary\n",
    "    print(f\"\\nDR class breakdown:\")\n",
    "    dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "    for dr_class in range(5):\n",
    "        dr_stats = stats['by_dr_class'][dr_class]\n",
    "        total_dr = dr_stats['kept'] + dr_stats['removed']\n",
    "        if total_dr > 0:\n",
    "            removal_rate = (dr_stats['removed'] / total_dr) * 100\n",
    "            print(f\"  {dr_names[dr_class]}: {dr_stats['kept']:,} kept, \"\n",
    "                  f\"{dr_stats['removed']:,} removed ({removal_rate:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nOutput location: {output_dir}\")\n",
    "    print(f\"Statistics saved: {output_dir}/cleaning_statistics.json\")\n",
    "\n",
    "# Execute dataset cleaning\n",
    "if __name__ == \"__main__\" or 'valid_datasets' in locals():\n",
    "    print(\"Dataset Cleaning with 3-Component Quality Scoring\")\n",
    "    \n",
    "    # Check for required variables\n",
    "    if 'valid_datasets' not in locals() or 'dataset_profiles' not in locals():\n",
    "        print(\"Error: Required variables not found (valid_datasets, dataset_profiles)\")\n",
    "        print(\"Ensure previous analysis steps are completed\")\n",
    "    else:\n",
    "        print(\"Processing may take time depending on dataset size\")\n",
    "        \n",
    "        # Execute cleaning\n",
    "        final_stats, total_processing_time = create_cleaned_dataset(\n",
    "            valid_datasets, dataset_profiles\n",
    "        )\n",
    "        \n",
    "        # Generate report\n",
    "        generate_cleaning_report(final_stats, total_processing_time, 'DREAM_dataset_cleaned')\n",
    "        \n",
    "        print(\"Dataset cleaning completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ce149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Quality Analysis CSV Generation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_quality_csv(all_results, output_file='quality_analysis.csv'):\n",
    "    \"\"\"\n",
    "    Generate comprehensive quality analysis CSV with all metrics and assessments.\n",
    "    \n",
    "    Args:\n",
    "        all_results: List of quality analysis results from image processing\n",
    "        output_file: Output CSV file path\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Complete analysis dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    dr_names = {0: 'No DR', 1: 'Mild DR', 2: 'Moderate DR', 3: 'Severe DR', 4: 'Proliferative DR'}\n",
    "    analysis_data = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        width, height = result.get('resolution', (0, 0))\n",
    "        \n",
    "        # Format removal reasons\n",
    "        removal_reasons = result.get('removal_reasons', [])\n",
    "        if isinstance(removal_reasons, list):\n",
    "            removal_reasons_str = ';'.join(removal_reasons) if removal_reasons else ''\n",
    "        else:\n",
    "            removal_reasons_str = str(removal_reasons)\n",
    "        \n",
    "        analysis_record = {\n",
    "            # Image identification\n",
    "            'dataset_name': result.get('dataset', ''),\n",
    "            'filename': result.get('filename', ''),\n",
    "            'dr_severity': result.get('dr_severity', 0),\n",
    "            'dr_class_name': dr_names.get(result.get('dr_severity', 0), 'Unknown'),\n",
    "            \n",
    "            # Quality scores\n",
    "            'overall_quality_score': round(result.get('overall_quality_score', 0), 4),\n",
    "            'basic_quality_score': round(result.get('basic_quality_score', 0), 4),\n",
    "            'medical_quality_score': round(result.get('medical_quality_score', 0), 4),\n",
    "            'technical_quality_score': round(result.get('technical_quality_score', 0), 4),\n",
    "            \n",
    "            # Core quality metrics\n",
    "            'brightness': round(result.get('brightness', 0), 2),\n",
    "            'contrast': round(result.get('contrast', 0), 2),\n",
    "            'sharpness': round(result.get('sharpness', 0), 1),\n",
    "            'entropy': round(result.get('entropy', 0), 3),\n",
    "            'illumination_uniformity': round(result.get('illumination_uniformity', 0), 4),\n",
    "            \n",
    "            # Medical-specific metrics\n",
    "            'vessel_visibility': round(result.get('vessel_visibility', 0), 4),\n",
    "            'optic_disc_visibility': round(result.get('optic_disc_visibility', 0), 4),\n",
    "            'color_balance': round(result.get('color_balance', 0), 2),\n",
    "            \n",
    "            # Assessment results\n",
    "            'recommended_action': result.get('recommended_action', 'UNKNOWN'),\n",
    "            'confidence': result.get('confidence', 'UNKNOWN'),\n",
    "            'removal_reasons': removal_reasons_str,\n",
    "            'threshold_used': round(result.get('threshold_used', 0), 4),\n",
    "            \n",
    "            # Technical metadata\n",
    "            'image_width': width,\n",
    "            'image_height': height,\n",
    "            'file_size_mb': round(result.get('file_size_mb', 0), 2),\n",
    "            'extreme_brightness_pixels': round(result.get('extreme_brightness_pixels', 0), 4),\n",
    "            'motion_blur_score': round(result.get('motion_blur_score', 0), 2)\n",
    "        }\n",
    "        \n",
    "        analysis_data.append(analysis_record)\n",
    "    \n",
    "    df = pd.DataFrame(analysis_data)\n",
    "    df = df.sort_values(['dataset_name', 'dr_severity', 'overall_quality_score'], \n",
    "                       ascending=[True, True, False])\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Quality analysis saved: {output_file} ({len(df):,} records, {len(df.columns)} columns)\")\n",
    "    return df\n",
    "\n",
    "def create_summary_statistics(df, output_file='summary_statistics.csv'):\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary statistics from quality analysis data.\n",
    "    \n",
    "    Args:\n",
    "        df: Quality analysis DataFrame\n",
    "        output_file: Base filename for output files\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (dataset_stats, dr_stats, metric_stats)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_images = len(df)\n",
    "    images_kept = len(df[df['recommended_action'] == 'KEEP'])\n",
    "    images_removed = len(df[df['recommended_action'] == 'REMOVE'])\n",
    "    removal_rate = round(images_removed / total_images * 100, 1)\n",
    "    \n",
    "    overall_stats = {\n",
    "        'Metric': ['Total Images', 'Images Kept', 'Images Removed', 'Removal Rate (%)'],\n",
    "        'Value': [total_images, images_kept, images_removed, removal_rate]\n",
    "    }\n",
    "    \n",
    "    # Dataset-level statistics\n",
    "    dataset_stats = []\n",
    "    for dataset in df['dataset_name'].unique():\n",
    "        dataset_data = df[df['dataset_name'] == dataset]\n",
    "        removed = len(dataset_data[dataset_data['recommended_action'] == 'REMOVE'])\n",
    "        total = len(dataset_data)\n",
    "        \n",
    "        dataset_stats.append({\n",
    "            'Dataset': dataset,\n",
    "            'Total_Images': total,\n",
    "            'Images_Kept': total - removed,\n",
    "            'Images_Removed': removed,\n",
    "            'Removal_Rate_Percent': round(removed / total * 100, 1),\n",
    "            'Mean_Quality_Score': round(dataset_data['overall_quality_score'].mean(), 3),\n",
    "            'Std_Quality_Score': round(dataset_data['overall_quality_score'].std(), 3)\n",
    "        })\n",
    "    \n",
    "    # DR severity statistics\n",
    "    dr_stats = []\n",
    "    for dr_class in sorted(df['dr_severity'].unique()):\n",
    "        dr_data = df[df['dr_severity'] == dr_class]\n",
    "        removed = len(dr_data[dr_data['recommended_action'] == 'REMOVE'])\n",
    "        total = len(dr_data)\n",
    "        \n",
    "        dr_stats.append({\n",
    "            'DR_Severity': dr_class,\n",
    "            'DR_Class_Name': dr_data['dr_class_name'].iloc[0],\n",
    "            'Total_Images': total,\n",
    "            'Images_Kept': total - removed,\n",
    "            'Images_Removed': removed,\n",
    "            'Removal_Rate_Percent': round(removed / total * 100, 1),\n",
    "            'Mean_Quality_Score': round(dr_data['overall_quality_score'].mean(), 3),\n",
    "            'Std_Quality_Score': round(dr_data['overall_quality_score'].std(), 3)\n",
    "        })\n",
    "    \n",
    "    # Quality metric descriptive statistics\n",
    "    quality_metrics = ['brightness', 'contrast', 'sharpness', 'entropy', \n",
    "                      'illumination_uniformity', 'vessel_visibility', 'optic_disc_visibility']\n",
    "    \n",
    "    metric_stats = []\n",
    "    for metric in quality_metrics:\n",
    "        if metric in df.columns:\n",
    "            metric_stats.append({\n",
    "                'Metric': metric,\n",
    "                'Mean': round(df[metric].mean(), 4),\n",
    "                'Std': round(df[metric].std(), 4),\n",
    "                'Min': round(df[metric].min(), 4),\n",
    "                'Max': round(df[metric].max(), 4),\n",
    "                'Q25': round(df[metric].quantile(0.25), 4),\n",
    "                'Q50': round(df[metric].quantile(0.50), 4),\n",
    "                'Q75': round(df[metric].quantile(0.75), 4)\n",
    "            })\n",
    "    \n",
    "    # Save statistics\n",
    "    output_excel = output_file.replace('.csv', '.xlsx')\n",
    "    \n",
    "    try:\n",
    "        with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:\n",
    "            pd.DataFrame(overall_stats).to_excel(writer, sheet_name='Overall_Stats', index=False)\n",
    "            pd.DataFrame(dataset_stats).to_excel(writer, sheet_name='Dataset_Stats', index=False)\n",
    "            pd.DataFrame(dr_stats).to_excel(writer, sheet_name='DR_Severity_Stats', index=False)\n",
    "            pd.DataFrame(metric_stats).to_excel(writer, sheet_name='Quality_Metrics_Stats', index=False)\n",
    "        \n",
    "        print(f\"Summary statistics saved: {output_excel}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback to separate CSV files\n",
    "        base_name = output_file.replace('.csv', '')\n",
    "        pd.DataFrame(overall_stats).to_csv(f'{base_name}_overall.csv', index=False)\n",
    "        pd.DataFrame(dataset_stats).to_csv(f'{base_name}_datasets.csv', index=False)\n",
    "        pd.DataFrame(dr_stats).to_csv(f'{base_name}_dr_severity.csv', index=False)\n",
    "        pd.DataFrame(metric_stats).to_csv(f'{base_name}_quality_metrics.csv', index=False)\n",
    "        print(f\"Summary statistics saved as separate CSV files (openpyxl not available)\")\n",
    "    \n",
    "    return dataset_stats, dr_stats, metric_stats\n",
    "\n",
    "def generate_analysis_files():\n",
    "    \"\"\"Main function to generate quality analysis files.\"\"\"\n",
    "    \n",
    "    if 'all_results' not in locals() and 'all_results' not in globals():\n",
    "        print(\"Error: Quality analysis results not found\")\n",
    "        print(\"Required steps:\")\n",
    "        print(\"1. Run dataset validation\")\n",
    "        print(\"2. Run dataset characterization\") \n",
    "        print(\"3. Run quality issue identification\")\n",
    "        return None\n",
    "    \n",
    "    # Get results from global scope\n",
    "    results = globals().get('all_results', locals().get('all_results', []))\n",
    "    \n",
    "    if not results:\n",
    "        print(\"Error: No analysis results available\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Processing {len(results):,} quality analysis results\")\n",
    "    \n",
    "    # Generate main analysis CSV\n",
    "    analysis_df = create_quality_csv(results, 'quality_analysis.csv')\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    dataset_stats, dr_stats, metric_stats = create_summary_statistics(\n",
    "        analysis_df, 'summary_statistics.csv'\n",
    "    )\n",
    "    \n",
    "    # Display summary information\n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    for stat in dataset_stats:\n",
    "        print(f\"  {stat['Dataset']}: {stat['Total_Images']:,} images, \"\n",
    "              f\"{stat['Removal_Rate_Percent']}% removed\")\n",
    "    \n",
    "    print(f\"\\nDR Severity Summary:\")\n",
    "    for stat in dr_stats:\n",
    "        print(f\"  {stat['DR_Class_Name']}: {stat['Total_Images']:,} images, \"\n",
    "              f\"{stat['Removal_Rate_Percent']}% removed\")\n",
    "    \n",
    "    print(f\"\\nFiles generated:\")\n",
    "    print(f\"- quality_analysis.csv: Complete analysis dataset\")\n",
    "    print(f\"- summary_statistics.xlsx: Statistical summaries\")\n",
    "    \n",
    "    return analysis_df, dataset_stats, dr_stats, metric_stats\n",
    "\n",
    "# Execute analysis file generation\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Quality Analysis File Generation\")\n",
    "    \n",
    "    # Check for results availability\n",
    "    if 'all_results' in locals() and all_results:\n",
    "        generate_analysis_files()\n",
    "    else:\n",
    "        print(\"Quality analysis results not found. Ensure previous analysis steps are completed.\")\n",
    "else:\n",
    "    # When run as part of notebook\n",
    "    generate_analysis_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
