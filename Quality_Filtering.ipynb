{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01d9dd-ab62-4311-84ce-bfd1ede543c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup and Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "from collections import Counter, defaultdict\n",
    "import gc\n",
    "import random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Required packages loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ea6e5-f4d6-4df3-8f91-60e87cd2a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Path and Configuration\n",
    "\n",
    "BASE_PATH = \"Path/to/your/dataset\"  # Specify the base path to your dataset here\n",
    "\n",
    "# Save all the datasets to the BASE_PATH folder\n",
    "# Path configuration based on your folder structure\n",
    "datasets_config = {\n",
    "    'APTOS2019': f'{BASE_PATH}/APTOS 2019',\n",
    "    'Diabetic_Retinopathy_V03': f'{BASE_PATH}/Diabetic Retinopathy_V03',\n",
    "    'IDRiD': f'{BASE_PATH}/IDRiD',\n",
    "    'Messidor2': f'{BASE_PATH}/Messidor 2',\n",
    "    'SUSTech_SYSU': f'{BASE_PATH}/SUSTech_SYSU',\n",
    "    'DeepDRiD': f'{BASE_PATH}/DeepDRiD'\n",
    "}\n",
    "\n",
    "# Other configuration settings\n",
    "OUTPUT_DIR = 'quality_review'\n",
    "RANDOM_SEED = 42\n",
    "N_SAMPLES_PER_DATASET = 300  # Number of images to sample for characterization\n",
    "\n",
    "print(\"Configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c183499-77f9-4247-bc5a-83b93c49441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Validate Dataset Paths\n",
    "\n",
    "def validate_dataset_paths(datasets_config):\n",
    "    valid_datasets = {}\n",
    "    \n",
    "    for name, path in datasets_config.items():\n",
    "        logger.info(f\"Checking {name}\")\n",
    "        logger.info(f\"Path: {path}\")\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            logger.error(f\"Path does not exist: {path}\")\n",
    "            continue\n",
    "            \n",
    "        # Check for DR class folders (0, 1, 2, 3, 4)\n",
    "        dr_folders = []\n",
    "        image_counts = {}\n",
    "        \n",
    "        try:\n",
    "            for item in os.listdir(path):\n",
    "                item_path = os.path.join(path, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    if item.isdigit() and int(item) in [0, 1, 2, 3, 4]:\n",
    "                        dr_class = int(item)\n",
    "                        dr_folders.append(dr_class)\n",
    "                        image_extensions = ('.jpg', '.jpeg', '.png', '.tiff', '.bmp', '.JPG', '.JPEG', '.PNG')\n",
    "                        image_count = 0\n",
    "                        for root, dirs, files in os.walk(item_path):\n",
    "                            for file in files:\n",
    "                                if file.lower().endswith(image_extensions):\n",
    "                                    image_count += 1\n",
    "                        image_counts[dr_class] = image_count\n",
    "            \n",
    "            if dr_folders:\n",
    "                dr_folders.sort()\n",
    "                total_images = sum(image_counts.values())\n",
    "                logger.info(f\"Found DR classes: {dr_folders}\")\n",
    "                logger.info(f\"Image counts per class:\")\n",
    "                dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "                for dr_class in dr_folders:\n",
    "                    dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "                    logger.info(f\"  {dr_name} (Class {dr_class}): {image_counts[dr_class]:,} images\")\n",
    "                logger.info(f\"Total images: {total_images:,}\")\n",
    "                \n",
    "                if total_images > 0:\n",
    "                    valid_datasets[name] = path\n",
    "                else:\n",
    "                    logger.warning(f\"No images found in {name}\")\n",
    "            else:\n",
    "                logger.warning(f\"No DR class folders (0,1,2,3,4) found in {name}\")\n",
    "                available_folders = [item for item in os.listdir(path) if os.path.isdir(os.path.join(path, item))]\n",
    "                logger.info(f\"Available folders: {available_folders}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error accessing path {path}: {e}\")\n",
    "\n",
    "    return valid_datasets\n",
    "\n",
    "# Run validation\n",
    "logger.info(\"Starting dataset validation\")\n",
    "valid_datasets = validate_dataset_paths(datasets_config)\n",
    "\n",
    "if not valid_datasets:\n",
    "    logger.error(\"No valid datasets found\")\n",
    "    logger.info(\"Setup instructions:\")\n",
    "    logger.info(\"1. Update BASE_PATH in cell [2] to your actual dataset location\")\n",
    "    logger.info(\"2. Ensure your datasets have folders named 0, 1, 2, 3, 4 containing images\")\n",
    "else:\n",
    "    logger.info(f\"Dataset validation completed - {len(valid_datasets)} datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b4678-122c-479c-859d-964c1a222d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Quality Identifiers Class\n",
    "\n",
    "class QualityIdentifier:\n",
    "    def __init__(self, output_dir='quality_review', random_seed=42):\n",
    "        self.output_dir = output_dir\n",
    "        self.dataset_profiles = {}\n",
    "        self.identification_results = []\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(f'{output_dir}/sample_images', exist_ok=True)\n",
    "        os.makedirs(f'{output_dir}/flagged_samples', exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Quality identification initialized - output directory: {output_dir}\")\n",
    "    \n",
    "    def extract_dr_label_from_path(self, image_path):\n",
    "        parts = image_path.split(os.sep)\n",
    "        \n",
    "        for part in parts:\n",
    "            if part.isdigit() and int(part) in [0, 1, 2, 3, 4]:\n",
    "                return int(part)\n",
    "        \n",
    "        dr_patterns = {\n",
    "            'no_dr': 0, 'normal': 0, 'grade_0': 0, 'class_0': 0,\n",
    "            'mild': 1, 'grade_1': 1, 'class_1': 1,\n",
    "            'moderate': 2, 'grade_2': 2, 'class_2': 2,\n",
    "            'severe': 3, 'grade_3': 3, 'class_3': 3,\n",
    "            'proliferative': 4, 'grade_4': 4, 'class_4': 4\n",
    "        }\n",
    "        \n",
    "        for part in parts:\n",
    "            part_lower = part.lower()\n",
    "            if part_lower in dr_patterns:\n",
    "                return dr_patterns[part_lower]\n",
    "        return None\n",
    "    \n",
    "    def sample_images_strategically(self, dataset_path, n_samples):\n",
    "        all_images = []\n",
    "        class_images = {0: [], 1: [], 2: [], 3: [], 4: []}\n",
    "        \n",
    "        image_extensions = ('.jpg', '.jpeg', '.png', '.tiff', '.bmp', '.JPG', '.JPEG', '.PNG')\n",
    "        \n",
    "        for root, dirs, files in os.walk(dataset_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(image_extensions):\n",
    "                    image_path = os.path.join(root, file)\n",
    "                    dr_class = self.extract_dr_label_from_path(image_path)\n",
    "                    if dr_class is not None:\n",
    "                        class_images[dr_class].append(image_path)\n",
    "        \n",
    "        for dr_class, images in class_images.items():\n",
    "            dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "            dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "            if images:\n",
    "                logger.info(f\"    {dr_name}: {len(images)} images\")\n",
    "        \n",
    "        sampled_images = []\n",
    "        total_available = sum(len(images) for images in class_images.values())\n",
    "        \n",
    "        if total_available == 0:\n",
    "            logger.warning(\"No images found with valid DR labels\")\n",
    "            return []\n",
    "        \n",
    "        samples_per_class = max(1, n_samples // 5)\n",
    "        \n",
    "        for dr_class, images in class_images.items():\n",
    "            if images:\n",
    "                n_class_samples = min(samples_per_class, len(images))\n",
    "                sampled = np.random.choice(images, n_class_samples, replace=False)\n",
    "                sampled_images.extend(sampled)\n",
    "\n",
    "        logger.info(f\"Selected {len(sampled_images)} stratified samples\")\n",
    "        return sampled_images\n",
    "    \n",
    "    def safe_calculate_brightness(self, image):\n",
    "        try:\n",
    "            brightness = float(np.mean(image))\n",
    "            return max(0.0, min(255.0, brightness))\n",
    "        except:\n",
    "            return 127.5\n",
    "    \n",
    "    def safe_calculate_contrast(self, image):\n",
    "        try:\n",
    "            contrast = float(np.std(image))\n",
    "            return max(0.0, contrast)\n",
    "        except:\n",
    "            return 50.0\n",
    "    \n",
    "    def safe_calculate_sharpness(self, gray):\n",
    "        try:\n",
    "            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "            return max(0.0, float(laplacian_var))\n",
    "        except:\n",
    "            return 500.0\n",
    "    \n",
    "    def safe_calculate_entropy(self, gray):\n",
    "        try:\n",
    "            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "            hist = hist.flatten()\n",
    "            hist = hist[hist > 0]\n",
    "            if len(hist) == 0:\n",
    "                return 4.0\n",
    "            prob = hist / hist.sum()\n",
    "            entropy = float(-np.sum(prob * np.log2(prob)))\n",
    "            return max(0.0, min(8.0, entropy))\n",
    "        except:\n",
    "            return 4.0\n",
    "\n",
    "    def assess_vessel_visibility_improved(self, image):\n",
    "        try:\n",
    "            green = image[:, :, 1]\n",
    "            \n",
    "            kernels = {\n",
    "                'horizontal': np.array([[-1, -1, -1], [2, 2, 2], [-1, -1, -1]], dtype=np.float32),\n",
    "                'vertical': np.array([[-1, 2, -1], [-1, 2, -1], [-1, 2, -1]], dtype=np.float32),\n",
    "                'diagonal1': np.array([[2, -1, -1], [-1, 2, -1], [-1, -1, 2]], dtype=np.float32),\n",
    "                'diagonal2': np.array([[-1, -1, 2], [-1, 2, -1], [2, -1, -1]], dtype=np.float32)\n",
    "            }\n",
    "            \n",
    "            vessel_responses = []\n",
    "            for kernel in kernels.values():\n",
    "                filtered = cv2.filter2D(green, cv2.CV_32F, kernel)\n",
    "                vessel_responses.append(filtered)\n",
    "            \n",
    "            max_response = np.maximum.reduce(vessel_responses)\n",
    "            threshold = np.percentile(max_response, 95)\n",
    "            vessel_pixels = np.sum(max_response > threshold)\n",
    "            total_pixels = max_response.shape[0] * max_response.shape[1]\n",
    "            \n",
    "            visibility_score = vessel_pixels / max(total_pixels, 1)\n",
    "            return float(min(0.1, visibility_score))\n",
    "            \n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def assess_optic_disc_visibility(self, image):\n",
    "        try:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            bright_pixels = np.sum(gray > np.percentile(gray, 90))\n",
    "            total_pixels = gray.shape[0] * gray.shape[1]\n",
    "            return float(bright_pixels / max(total_pixels, 1))\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def assess_illumination_uniformity(self, gray):\n",
    "        try:\n",
    "            h, w = gray.shape\n",
    "            regions = []\n",
    "            \n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    start_y, end_y = i * h // 3, (i + 1) * h // 3\n",
    "                    start_x, end_x = j * w // 3, (j + 1) * w // 3\n",
    "                    region = gray[start_y:end_y, start_x:end_x]\n",
    "                    regions.append(np.mean(region))\n",
    "            \n",
    "            mean_intensity = np.mean(regions)\n",
    "            if mean_intensity > 0:\n",
    "                cv_score = np.std(regions) / mean_intensity\n",
    "                return float(max(0, min(1, 1 - cv_score)))\n",
    "            return 0.0\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def detect_extreme_pixels(self, gray):\n",
    "        try:\n",
    "            very_dark = np.sum(gray < 10)\n",
    "            very_bright = np.sum(gray > 245)\n",
    "            total_pixels = gray.shape[0] * gray.shape[1]\n",
    "            return float((very_dark + very_bright) / max(total_pixels, 1))\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def assess_motion_blur(self, gray):\n",
    "        try:\n",
    "            grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "            return float(np.mean(magnitude))\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def analyze_single_image(self, image_path):\n",
    "        try:\n",
    "            if not os.path.exists(image_path):\n",
    "                logger.error(f\"File not found: {image_path}\")\n",
    "                return None\n",
    "                \n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                logger.error(f\"Could not read image: {image_path}\")\n",
    "                return None\n",
    "            \n",
    "            if len(image.shape) != 3:\n",
    "                logger.error(f\"Invalid image format: {image_path}\")\n",
    "                return None\n",
    "                \n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            h, w = image.shape[:2]\n",
    "\n",
    "            if h < 100 or w < 100:\n",
    "                logger.error(f\"Image too small ({w}x{h}): {image_path}\")\n",
    "                return None\n",
    "            \n",
    "            characteristics = {\n",
    "                'image_path': image_path,\n",
    "                'filename': os.path.basename(image_path),\n",
    "                'resolution': (w, h),\n",
    "                'file_size_mb': max(0.001, os.path.getsize(image_path) / (1024*1024)),\n",
    "                \n",
    "                'brightness': self.safe_calculate_brightness(image),\n",
    "                'contrast': self.safe_calculate_contrast(image),\n",
    "                'sharpness': self.safe_calculate_sharpness(gray),\n",
    "                'entropy': self.safe_calculate_entropy(gray),\n",
    "                \n",
    "                'color_balance': float(np.std([np.mean(image[:,:,0]), np.mean(image[:,:,1]), np.mean(image[:,:,2])])),\n",
    "                \n",
    "                'vessel_visibility': self.assess_vessel_visibility_improved(image),\n",
    "                'optic_disc_visibility': self.assess_optic_disc_visibility(image),\n",
    "                'illumination_uniformity': self.assess_illumination_uniformity(gray),\n",
    "                \n",
    "                'extreme_brightness_pixels': self.detect_extreme_pixels(gray),\n",
    "                'motion_blur_score': self.assess_motion_blur(gray)\n",
    "            }\n",
    "            return characteristics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "logger.info(\"QualityIdentifier class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaace56-0745-4186-ab8a-7ddd39833f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize Quality Identifier\n",
    "\n",
    "# Proceed if we have datasets are validated in above cells\n",
    "if valid_datasets:\n",
    "    identifier = QualityIdentifier(\n",
    "        output_dir=OUTPUT_DIR, \n",
    "        random_seed=RANDOM_SEED\n",
    "    )\n",
    "    print(\"QualityIdentifier initialized!\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"Cannot initialize - no valid datasets found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe32452-0520-49ce-b7b8-e312f5aeaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Dataset Characterization & Adaptive Thresholding\n",
    "\n",
    "def characterize_dataset(identifier, dataset_path, dataset_name, n_samples=300):\n",
    "    logger.info(f\"Characterizing {dataset_name}\")\n",
    "    \n",
    "    sample_images = identifier.sample_images_strategically(dataset_path, n_samples)\n",
    "    \n",
    "    if not sample_images:\n",
    "        logger.warning(f\"No images found in {dataset_path}\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Analyzing {len(sample_images)} sample images\")\n",
    "    \n",
    "    characteristics = []\n",
    "    for i, img_path in enumerate(sample_images):\n",
    "        if i % 50 == 0 and i > 0:\n",
    "            progress = (i / len(sample_images)) * 100\n",
    "            logger.info(f\"Analysis progress: {progress:.1f}% ({i}/{len(sample_images)})\")\n",
    "        \n",
    "        char = identifier.analyze_single_image(img_path)\n",
    "        if char:\n",
    "            characteristics.append(char)  \n",
    "\n",
    "        if i % 100 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    if not characteristics:\n",
    "        logger.error(f\"No valid characteristics extracted from {dataset_name}\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Successfully analyzed {len(characteristics)} images\")\n",
    "\n",
    "    profile = calculate_dataset_profile(dataset_name, characteristics)\n",
    "    save_sample_images(identifier, sample_images[:20], dataset_name)\n",
    "    return profile\n",
    "\n",
    "def calculate_dataset_profile(dataset_name, characteristics):\n",
    "    \"\"\"Calculate dataset profile using standardized 3-component quality scoring\"\"\"\n",
    "    profile = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'analysis_date': datetime.now().isoformat(),\n",
    "        'n_samples_analyzed': len(characteristics),\n",
    "        'characteristics_stats': {},\n",
    "        'adaptive_thresholds': {}\n",
    "    }\n",
    "    \n",
    "    numeric_keys = [key for key in characteristics[0].keys() \n",
    "                   if key not in ['image_path', 'filename', 'resolution']]\n",
    "    \n",
    "    for key in numeric_keys:\n",
    "        raw_values = [char[key] for char in characteristics if char[key] is not None]\n",
    "        \n",
    "        values = []\n",
    "        for val in raw_values:\n",
    "            if isinstance(val, bool):\n",
    "                values.append(float(val))\n",
    "            elif isinstance(val, (int, float)):\n",
    "                values.append(float(val))\n",
    "        \n",
    "        if values:\n",
    "            profile['characteristics_stats'][key] = {\n",
    "                'mean': float(np.mean(values)),\n",
    "                'std': float(np.std(values)),\n",
    "                'min': float(np.min(values)),\n",
    "                'max': float(np.max(values)),\n",
    "                'percentiles': {\n",
    "                    '5': float(np.percentile(values, 5)),\n",
    "                    '10': float(np.percentile(values, 10)),\n",
    "                    '25': float(np.percentile(values, 25)),\n",
    "                    '50': float(np.percentile(values, 50)),\n",
    "                    '75': float(np.percentile(values, 75)),\n",
    "                    '90': float(np.percentile(values, 90)),\n",
    "                    '95': float(np.percentile(values, 95))\n",
    "                }\n",
    "            }\n",
    "\n",
    "    # Standardized adaptive thresholds using 3-component scoring\n",
    "    removal_percentiles = {0: 15, 1: 12, 2: 10, 3: 8, 4: 5}\n",
    "    \n",
    "    quality_scores = []\n",
    "    for char in characteristics:\n",
    "        try:\n",
    "            # Basic Quality Component\n",
    "            brightness = float(char['brightness']) if char['brightness'] is not None else 127.5\n",
    "            contrast = float(char['contrast']) if char['contrast'] is not None else 50.0\n",
    "            sharpness = float(char['sharpness']) if char['sharpness'] is not None else 500.0\n",
    "            entropy = float(char['entropy']) if char['entropy'] is not None else 4.0\n",
    "            \n",
    "            brightness_norm = min(1.0, max(0.0, brightness / 255.0))\n",
    "            contrast_norm = min(1.0, max(0.0, contrast / 100.0))\n",
    "            sharpness_norm = min(1.0, max(0.0, sharpness / 1000.0))\n",
    "            entropy_norm = min(1.0, max(0.0, entropy / 8.0))\n",
    "            basic_quality = np.mean([brightness_norm, contrast_norm, sharpness_norm, entropy_norm])\n",
    "            \n",
    "            # Medical Quality Component\n",
    "            illumination_uniformity = float(char['illumination_uniformity']) if char['illumination_uniformity'] is not None else 0.5\n",
    "            vessel_visibility = float(char['vessel_visibility']) if char['vessel_visibility'] is not None else 0.1\n",
    "            optic_disc_visibility = float(char['optic_disc_visibility']) if char['optic_disc_visibility'] is not None else 0.1\n",
    "            \n",
    "            medical_quality = np.mean([\n",
    "                illumination_uniformity,\n",
    "                min(1.0, vessel_visibility * 10),\n",
    "                min(1.0, optic_disc_visibility * 10)\n",
    "            ])\n",
    "            \n",
    "            # Technical Quality Component\n",
    "            extreme_pixels = float(char['extreme_brightness_pixels']) if char['extreme_brightness_pixels'] is not None else 0.1\n",
    "            motion_blur = float(char['motion_blur_score']) if char['motion_blur_score'] is not None else 20.0\n",
    "            color_balance = float(char['color_balance']) if char['color_balance'] is not None else 15.0\n",
    "            \n",
    "            extreme_pixels_norm = max(0, min(1, 1 - (extreme_pixels * 2)))\n",
    "            motion_blur_norm = min(1.0, max(0.0, motion_blur / 50.0))\n",
    "            color_balance_norm = max(0, min(1, 1 - (color_balance / 50.0)))\n",
    "            technical_quality = np.mean([extreme_pixels_norm, motion_blur_norm, color_balance_norm])\n",
    "            \n",
    "            # Composite Quality Score: 25% Basic + 55% Medical + 20% Technical\n",
    "            combined_quality = 0.25 * basic_quality + 0.55 * medical_quality + 0.20 * technical_quality\n",
    "            quality_scores.append(combined_quality)\n",
    "            \n",
    "        except (TypeError, ValueError) as e:\n",
    "            logger.warning(f\"Error calculating quality score: {e}\")\n",
    "            quality_scores.append(0.3)\n",
    "    \n",
    "    for dr_severity, percentile in removal_percentiles.items():\n",
    "        try:\n",
    "            threshold = np.percentile(quality_scores, percentile) if quality_scores else 0.3\n",
    "            profile['adaptive_thresholds'][dr_severity] = float(threshold)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating threshold for DR severity {dr_severity}: {e}\")\n",
    "            profile['adaptive_thresholds'][dr_severity] = 0.3\n",
    "\n",
    "    return profile  \n",
    "\n",
    "def save_sample_images(identifier, sample_paths, dataset_name):\n",
    "    sample_dir = f'{identifier.output_dir}/sample_images/{dataset_name}'\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    \n",
    "    copied_count = 0\n",
    "    for i, img_path in enumerate(sample_paths):\n",
    "        try:\n",
    "            dst_path = f'{sample_dir}/sample_{i:02d}_{os.path.basename(img_path)}'\n",
    "            shutil.copy2(img_path, dst_path)\n",
    "            copied_count += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error copying sample {img_path}: {e}\")\n",
    "\n",
    "    logger.info(f\"Saved {copied_count} sample images to {sample_dir}\")\n",
    "\n",
    "# Run characterization for all datasets\n",
    "if valid_datasets:\n",
    "    dataset_profiles = {}\n",
    "    \n",
    "    for dataset_name, dataset_path in valid_datasets.items():\n",
    "        profile = characterize_dataset(identifier, dataset_path, dataset_name, N_SAMPLES_PER_DATASET)\n",
    "        if profile:\n",
    "            dataset_profiles[dataset_name] = profile\n",
    "            identifier.dataset_profiles[dataset_name] = profile\n",
    "            logger.info(f\"{dataset_name} characterization completed\")\n",
    "\n",
    "            stats = profile['characteristics_stats']\n",
    "            if 'brightness' in stats:\n",
    "                logger.info(f\"Brightness: {stats['brightness']['mean']:.1f} ± {stats['brightness']['std']:.1f}\")\n",
    "            if 'sharpness' in stats:\n",
    "                logger.info(f\"Sharpness: {stats['sharpness']['mean']:.1f} ± {stats['sharpness']['std']:.1f}\")\n",
    "            if 'illumination_uniformity' in stats:\n",
    "                logger.info(f\"Illumination uniformity: {stats['illumination_uniformity']['mean']:.3f} ± {stats['illumination_uniformity']['std']:.3f}\")\n",
    "        else:\n",
    "            logger.error(f\"Failed characterization for {dataset_name}\")\n",
    "\n",
    "    logger.info(f\"Characterization completed for {len(dataset_profiles)} datasets\")\n",
    "else:\n",
    "    logger.error(\"Cannot proceed - no valid datasets found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe4d72-7516-431c-90d0-23b9e1290811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Quality Issue Identification\n",
    "\n",
    "def assess_image_quality_corrected(characteristics, profile, dr_severity):\n",
    "    \"\"\"\n",
    "    Assess image quality using standardized 3-component scoring system.\n",
    "    Components: Basic Quality (25%), Medical Quality (55%), Technical Quality (20%)\n",
    "    \"\"\"\n",
    "    char_stats = profile['characteristics_stats']\n",
    "    normalized_scores = {}\n",
    "    \n",
    "    # Normalize technical metrics using z-score normalization\n",
    "    technical_metrics = ['brightness', 'contrast', 'sharpness', 'entropy']\n",
    "    for metric in technical_metrics:\n",
    "        if metric in char_stats and metric in characteristics:\n",
    "            stats = char_stats[metric]\n",
    "            value = characteristics[metric]\n",
    "            \n",
    "            if stats['std'] > 0:\n",
    "                z_score = (value - stats['mean']) / stats['std']\n",
    "                normalized_scores[metric] = max(0, min(1, (z_score + 3) / 6))\n",
    "            else:\n",
    "                normalized_scores[metric] = 0.5\n",
    "    \n",
    "    # Direct normalization for medical metrics\n",
    "    medical_metrics = {\n",
    "        'illumination_uniformity': lambda x: min(1.0, max(0.0, x)),\n",
    "        'vessel_visibility': lambda x: min(1.0, max(0.0, x * 100)),\n",
    "        'optic_disc_visibility': lambda x: min(1.0, max(0.0, x * 50))\n",
    "    }\n",
    "    \n",
    "    for metric, normalizer in medical_metrics.items():\n",
    "        if metric in characteristics:\n",
    "            normalized_scores[metric] = normalizer(characteristics[metric])\n",
    "    \n",
    "    # Technical quality metrics normalization\n",
    "    if 'extreme_brightness_pixels' in characteristics:\n",
    "        ep_value = characteristics['extreme_brightness_pixels']\n",
    "        normalized_scores['extreme_brightness_pixels'] = max(0, min(1, 1 - (ep_value * 2)))\n",
    "    \n",
    "    if 'motion_blur_score' in characteristics:\n",
    "        mb_value = characteristics['motion_blur_score']\n",
    "        normalized_scores['motion_blur_score'] = min(1.0, max(0.0, mb_value / 50.0))\n",
    "    \n",
    "    if 'color_balance' in characteristics:\n",
    "        cb_value = characteristics['color_balance']\n",
    "        normalized_scores['color_balance'] = max(0, min(1, 1 - (cb_value / 50.0)))\n",
    "    \n",
    "    # Calculate component scores\n",
    "    basic_metrics = ['brightness', 'contrast', 'sharpness', 'entropy']\n",
    "    medical_metrics_list = ['illumination_uniformity', 'vessel_visibility', 'optic_disc_visibility']\n",
    "    technical_metrics_list = ['extreme_brightness_pixels', 'motion_blur_score', 'color_balance']\n",
    "    \n",
    "    basic_score = np.mean([normalized_scores.get(m, 0.5) for m in basic_metrics])\n",
    "    medical_score = np.mean([normalized_scores.get(m, 0.5) for m in medical_metrics_list])\n",
    "    technical_score = np.mean([normalized_scores.get(m, 0.5) for m in technical_metrics_list])\n",
    "    \n",
    "    # Composite Quality Score: 25% Basic + 55% Medical + 20% Technical\n",
    "    overall_score = 0.25 * basic_score + 0.55 * medical_score + 0.20 * technical_score\n",
    "    \n",
    "    threshold = profile['adaptive_thresholds'].get(dr_severity, 0.3)\n",
    "    \n",
    "    # Critical quality assessment\n",
    "    removal_reasons = []\n",
    "    \n",
    "    if characteristics['sharpness'] < char_stats.get('sharpness', {}).get('percentiles', {}).get('5', 0):\n",
    "        removal_reasons.append('extremely_blurry')\n",
    "    \n",
    "    if characteristics['brightness'] < 20 or characteristics['brightness'] > 240:\n",
    "        removal_reasons.append('extreme_brightness')\n",
    "    \n",
    "    if characteristics['illumination_uniformity'] < 0.1:\n",
    "        removal_reasons.append('poor_illumination')\n",
    "    \n",
    "    if characteristics['vessel_visibility'] < char_stats.get('vessel_visibility', {}).get('percentiles', {}).get('10', 0):\n",
    "        removal_reasons.append('poor_vessel_visibility')\n",
    "    \n",
    "    if characteristics['extreme_brightness_pixels'] > 0.3:\n",
    "        removal_reasons.append('too_many_extreme_pixels')\n",
    "    \n",
    "    if characteristics['file_size_mb'] < 0.1:\n",
    "        removal_reasons.append('file_too_small')\n",
    "    \n",
    "    w, h = characteristics['resolution']\n",
    "    if w < 224 or h < 224:\n",
    "        removal_reasons.append('resolution_too_low')\n",
    "    \n",
    "    if characteristics.get('motion_blur_score', 0) < 5:\n",
    "        removal_reasons.append('severe_motion_blur')\n",
    "    \n",
    "    if characteristics.get('color_balance', 0) > 40:\n",
    "        removal_reasons.append('severe_color_imbalance')\n",
    "    \n",
    "    # Final decision logic\n",
    "    if len(removal_reasons) >= 2:\n",
    "        action = 'REMOVE'\n",
    "        confidence = 'HIGH'\n",
    "    elif overall_score < threshold:\n",
    "        action = 'REMOVE'\n",
    "        confidence = 'MEDIUM'\n",
    "    else:\n",
    "        action = 'KEEP'\n",
    "        confidence = 'HIGH' if overall_score > threshold + 0.1 else 'MEDIUM'\n",
    "    \n",
    "    return {\n",
    "        'overall_score': overall_score,\n",
    "        'basic_score': basic_score,\n",
    "        'medical_score': medical_score, \n",
    "        'technical_score': technical_score,\n",
    "        'threshold': threshold,\n",
    "        'action': action,\n",
    "        'reasons': removal_reasons,\n",
    "        'confidence': confidence,\n",
    "        'normalized_scores': normalized_scores\n",
    "    }\n",
    "\n",
    "def identify_quality_issues(identifier, dataset_path, dataset_name, profile):\n",
    "    logger.info(f\"Identifying quality issues in {dataset_name}\")\n",
    "    \n",
    "    results = []\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    image_extensions = ('.jpg', '.jpeg', '.png', '.tiff', '.bmp', '.JPG', '.JPEG', '.PNG')\n",
    "    \n",
    "    # Count total images for progress tracking\n",
    "    total_images = 0\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(image_extensions):\n",
    "                image_path = os.path.join(root, file)\n",
    "                dr_severity = identifier.extract_dr_label_from_path(image_path)\n",
    "                if dr_severity is not None:\n",
    "                    total_images += 1\n",
    "    \n",
    "    logger.info(f\"Processing {total_images:,} images\")\n",
    "    \n",
    "    # Process images\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(image_extensions):\n",
    "                image_path = os.path.join(root, file)\n",
    "                dr_severity = identifier.extract_dr_label_from_path(image_path)\n",
    "                \n",
    "                if dr_severity is None:\n",
    "                    continue\n",
    "                \n",
    "                characteristics = identifier.analyze_single_image(image_path)\n",
    "                if not characteristics:\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                quality_assessment = assess_image_quality_corrected(characteristics, profile, dr_severity)\n",
    "                \n",
    "                result = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'image_path': image_path,\n",
    "                    'filename': file,\n",
    "                    'dr_severity': dr_severity,\n",
    "                    'overall_quality_score': quality_assessment['overall_score'],\n",
    "                    'basic_quality_score': quality_assessment['basic_score'],\n",
    "                    'medical_quality_score': quality_assessment['medical_score'],\n",
    "                    'technical_quality_score': quality_assessment['technical_score'],\n",
    "                    'threshold_used': quality_assessment['threshold'],\n",
    "                    'recommended_action': quality_assessment['action'],\n",
    "                    'removal_reasons': quality_assessment['reasons'],\n",
    "                    'confidence': quality_assessment['confidence']\n",
    "                }\n",
    "                \n",
    "                result.update(characteristics)\n",
    "                result.update({f'normalized_{k}': v for k, v in quality_assessment['normalized_scores'].items()})\n",
    "                \n",
    "                results.append(result)\n",
    "                processed_count += 1\n",
    "                \n",
    "                if processed_count % 1000 == 0:\n",
    "                    progress = (processed_count / total_images) * 100 if total_images > 0 else 0\n",
    "                    logger.info(f\"Processing progress: {progress:.1f}% ({processed_count:,}/{total_images:,})\")\n",
    "                    gc.collect()\n",
    "    \n",
    "    logger.info(f\"Analysis completed: {processed_count:,} images processed\")\n",
    "    if error_count > 0:\n",
    "        logger.warning(f\"Images with errors: {error_count}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute quality issue identification\n",
    "if valid_datasets and dataset_profiles:\n",
    "    all_results = []\n",
    "    \n",
    "    for dataset_name, profile in dataset_profiles.items():\n",
    "        logger.info(f\"Processing quality assessment for {dataset_name}\")\n",
    "        dataset_path = valid_datasets[dataset_name]\n",
    "        results = identify_quality_issues(identifier, dataset_path, dataset_name, profile)\n",
    "        all_results.extend(results)\n",
    "        \n",
    "        flagged = [r for r in results if r['recommended_action'] == 'REMOVE']\n",
    "        flagged_count = len(flagged)\n",
    "        total_count = len(results)\n",
    "        removal_rate = flagged_count / total_count * 100 if total_count > 0 else 0\n",
    "        \n",
    "        logger.info(f\"{dataset_name} - Total: {total_count:,}, Flagged: {flagged_count:,} ({removal_rate:.1f}%)\")\n",
    "        \n",
    "        if results:\n",
    "            avg_basic = np.mean([r['basic_quality_score'] for r in results])\n",
    "            avg_medical = np.mean([r['medical_quality_score'] for r in results])\n",
    "            avg_technical = np.mean([r['technical_quality_score'] for r in results])\n",
    "            avg_overall = np.mean([r['overall_quality_score'] for r in results])\n",
    "            \n",
    "            logger.info(f\"Average scores - Basic: {avg_basic:.3f}, Medical: {avg_medical:.3f}, Technical: {avg_technical:.3f}, Overall: {avg_overall:.3f}\")\n",
    "        \n",
    "        # DR class breakdown\n",
    "        dr_stats = defaultdict(lambda: {'total': 0, 'flagged': 0})\n",
    "        for result in results:\n",
    "            dr_class = result['dr_severity']\n",
    "            dr_stats[dr_class]['total'] += 1\n",
    "            if result['recommended_action'] == 'REMOVE':\n",
    "                dr_stats[dr_class]['flagged'] += 1\n",
    "        \n",
    "        dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "        for dr_class in sorted(dr_stats.keys()):\n",
    "            stats = dr_stats[dr_class]\n",
    "            class_removal_rate = stats['flagged'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "            dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "            logger.info(f\"{dr_name}: {stats['flagged']:,}/{stats['total']:,} ({class_removal_rate:.1f}%)\")\n",
    "    \n",
    "    logger.info(f\"Quality identification completed for {len(all_results):,} images\")\n",
    "    \n",
    "    if all_results:\n",
    "        total_flagged = len([r for r in all_results if r['recommended_action'] == 'REMOVE'])\n",
    "        overall_removal_rate = total_flagged / len(all_results) * 100\n",
    "        \n",
    "        avg_basic_all = np.mean([r['basic_quality_score'] for r in all_results])\n",
    "        avg_medical_all = np.mean([r['medical_quality_score'] for r in all_results])\n",
    "        avg_technical_all = np.mean([r['technical_quality_score'] for r in all_results])\n",
    "        avg_overall_all = np.mean([r['overall_quality_score'] for r in all_results])\n",
    "        \n",
    "        logger.info(f\"Overall removal rate: {overall_removal_rate:.1f}%\")\n",
    "        logger.info(f\"Average quality scores - Basic: {avg_basic_all:.3f}, Medical: {avg_medical_all:.3f}, Technical: {avg_technical_all:.3f}, Overall: {avg_overall_all:.3f}\")\n",
    "        \n",
    "else:\n",
    "    logger.error(\"Quality identification skipped - missing datasets or profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59dd37e-843e-419f-9faa-f75c714c640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create Flagged Samples\n",
    "\n",
    "def create_flagged_samples(identifier, results, n_samples_per_dataset=20):\n",
    "    by_dataset = {}\n",
    "    for result in results:\n",
    "        dataset = result['dataset']\n",
    "        if dataset not in by_dataset:\n",
    "            by_dataset[dataset] = []\n",
    "        by_dataset[dataset].append(result)\n",
    "   \n",
    "    total_samples_created = 0\n",
    "   \n",
    "    for dataset_name, dataset_results in by_dataset.items():\n",
    "        logger.info(f\"Processing flagged samples for {dataset_name}\")\n",
    "       \n",
    "        flagged = [r for r in dataset_results if r['recommended_action'] == 'REMOVE']\n",
    "       \n",
    "        if not flagged:\n",
    "            logger.info(f\"No flagged images found for {dataset_name}\")\n",
    "            continue\n",
    "       \n",
    "        logger.info(f\"Found {len(flagged)} flagged images\")\n",
    "       \n",
    "        sample_dir = f'{identifier.output_dir}/flagged_samples/{dataset_name}'\n",
    "        os.makedirs(sample_dir, exist_ok=True)\n",
    "       \n",
    "        # Group by removal reasons\n",
    "        by_reason = {}\n",
    "        for result in flagged:\n",
    "            for reason in result['removal_reasons']:\n",
    "                if reason not in by_reason:\n",
    "                    by_reason[reason] = []\n",
    "                by_reason[reason].append(result)\n",
    "       \n",
    "        logger.info(f\"Issue types found: {list(by_reason.keys())}\")\n",
    "       \n",
    "        # Sample for each category\n",
    "        samples_copied = 0\n",
    "        for reason, reason_results in by_reason.items():\n",
    "            reason_samples = min(5, len(reason_results), n_samples_per_dataset - samples_copied)\n",
    "            if reason_samples <= 0:\n",
    "                continue\n",
    "           \n",
    "            # Sort by confidence and take most confident removals\n",
    "            reason_results.sort(key=lambda x: x['overall_quality_score'])\n",
    "           \n",
    "            for i, result in enumerate(reason_results[:reason_samples]):\n",
    "                try:\n",
    "                    src_path = result['image_path']\n",
    "                    dst_filename = f'{reason}_{i:02d}_{result[\"filename\"]}'\n",
    "                    dst_path = os.path.join(sample_dir, dst_filename)\n",
    "                    shutil.copy2(src_path, dst_path)\n",
    "                    samples_copied += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error copying flagged sample {src_path}: {e}\")\n",
    "       \n",
    "        logger.info(f\"Created {samples_copied} flagged samples for {dataset_name}\")\n",
    "        total_samples_created += samples_copied\n",
    "   \n",
    "    logger.info(f\"Total flagged samples created: {total_samples_created}\")\n",
    "\n",
    "# Run flagged sample creation\n",
    "if 'all_results' in locals() and all_results:\n",
    "    create_flagged_samples(identifier, all_results)\n",
    "else:\n",
    "    logger.warning(\"Flagged sample creation skipped - no results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e0b85-fab8-46ef-940b-651da37b3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Flagged Images Report Generation\n",
    "\n",
    "def generate_identification_report(all_results):\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_images = len(df)\n",
    "    flagged_for_removal = len(df[df['recommended_action'] == 'REMOVE'])\n",
    "    removal_rate = flagged_for_removal / total_images if total_images > 0 else 0\n",
    "    \n",
    "    report = {\n",
    "        'analysis_summary': {\n",
    "            'total_images_analyzed': total_images,\n",
    "            'images_flagged_for_removal': flagged_for_removal,\n",
    "            'overall_removal_rate': removal_rate,\n",
    "            'analysis_date': datetime.now().isoformat()\n",
    "        },\n",
    "        'dataset_breakdown': {},\n",
    "        'removal_reasons_summary': {},\n",
    "        'quality_score_statistics': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Per dataset and class breakdown\n",
    "    for dataset in df['dataset'].unique():\n",
    "        dataset_data = df[df['dataset'] == dataset]\n",
    "        dataset_flagged = len(dataset_data[dataset_data['recommended_action'] == 'REMOVE'])\n",
    "        dataset_total = len(dataset_data)\n",
    "        \n",
    "        class_breakdown = {}\n",
    "        for dr_class in range(5):\n",
    "            class_data = dataset_data[dataset_data['dr_severity'] == dr_class]\n",
    "            if len(class_data) > 0:\n",
    "                class_flagged = len(class_data[class_data['recommended_action'] == 'REMOVE'])\n",
    "                class_breakdown[dr_class] = {\n",
    "                    'total': len(class_data),\n",
    "                    'flagged': class_flagged,\n",
    "                    'removal_rate': class_flagged / len(class_data)\n",
    "                }\n",
    "        \n",
    "        report['dataset_breakdown'][dataset] = {\n",
    "            'total_images': dataset_total,\n",
    "            'flagged_images': dataset_flagged,\n",
    "            'removal_rate': dataset_flagged / dataset_total if dataset_total > 0 else 0,\n",
    "            'class_breakdown': class_breakdown,\n",
    "            'avg_quality_score': float(dataset_data['overall_quality_score'].mean()),\n",
    "            'quality_score_std': float(dataset_data['overall_quality_score'].std())\n",
    "        }\n",
    "    \n",
    "    # Reasons summary\n",
    "    all_reasons = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['recommended_action'] == 'REMOVE':\n",
    "            all_reasons.extend(row['removal_reasons'])\n",
    "    reason_counts = Counter(all_reasons)\n",
    "    report['removal_reasons_summary'] = dict(reason_counts)\n",
    "    \n",
    "    # Statistics\n",
    "    report['quality_score_statistics'] = {\n",
    "        'mean': float(df['overall_quality_score'].mean()),\n",
    "        'std': float(df['overall_quality_score'].std()),\n",
    "        'min': float(df['overall_quality_score'].min()),\n",
    "        'max': float(df['overall_quality_score'].max()),\n",
    "        'percentiles': {\n",
    "            '10': float(df['overall_quality_score'].quantile(0.1)),\n",
    "            '25': float(df['overall_quality_score'].quantile(0.25)),\n",
    "            '50': float(df['overall_quality_score'].quantile(0.5)),\n",
    "            '75': float(df['overall_quality_score'].quantile(0.75)),\n",
    "            '90': float(df['overall_quality_score'].quantile(0.9))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Recommendations\n",
    "    if removal_rate > 0.4:\n",
    "        report['recommendations'].append(\"High removal rate detected - consider reducing quality thresholds\")\n",
    "    \n",
    "    if removal_rate < 0.05:\n",
    "        report['recommendations'].append(\"Low removal rate detected - consider tightening quality thresholds\")\n",
    "    \n",
    "    for dataset, stats in report['dataset_breakdown'].items():\n",
    "        if stats['removal_rate'] > 0.5:\n",
    "            report['recommendations'].append(f\"Very high removal rate for {dataset} - review dataset-specific thresholds\")\n",
    "        \n",
    "        # Check for class imbalance in removal\n",
    "        class_rates = [info['removal_rate'] for info in stats['class_breakdown'].values()]\n",
    "        if class_rates and max(class_rates) - min(class_rates) > 0.3:\n",
    "            report['recommendations'].append(f\"Uneven removal rates across DR classes in {dataset} - consider class-specific adjustments\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate report\n",
    "if 'all_results' in locals() and all_results:\n",
    "    report = generate_identification_report(all_results)\n",
    "    logger.info(\"Analysis report generated successfully\")\n",
    "else:\n",
    "    logger.warning(\"Report generation skipped - no results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19275b9a-c565-4fe8-babb-ebbd65210d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save Results\n",
    "\n",
    "def save_identification_results(identifier, results, report):\n",
    "    # Result report\n",
    "    df = pd.DataFrame(results)\n",
    "    results_file = f'{identifier.output_dir}/quality_identification_results.csv'\n",
    "    df.to_csv(results_file, index=False)\n",
    "   \n",
    "    # Profile report\n",
    "    profiles_file = f'{identifier.output_dir}/dataset_profiles.json'\n",
    "    with open(profiles_file, 'w') as f:\n",
    "        json.dump(identifier.dataset_profiles, f, indent=2)\n",
    "   \n",
    "    # Analysis report\n",
    "    report_file = f'{identifier.output_dir}/identification_report.json'\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "   \n",
    "    # Create summary CSV for flagged images\n",
    "    summary_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['recommended_action'] == 'REMOVE':\n",
    "            summary_data.append({\n",
    "                'dataset': row['dataset'],\n",
    "                'filename': row['filename'],\n",
    "                'dr_severity': row['dr_severity'],\n",
    "                'quality_score': row['overall_quality_score'],\n",
    "                'confidence': row['confidence'],\n",
    "                'reasons': ', '.join(row['removal_reasons']),\n",
    "                'image_path': row['image_path']\n",
    "            })\n",
    "   \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_file = f'{identifier.output_dir}/flagged_images_summary.csv'\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    logger.info(f\"Results saved successfully\")\n",
    "    logger.info(f\"Flagged summary: {summary_file}\")\n",
    "    logger.info(f\"Complete results: {results_file}\")\n",
    "    logger.info(f\"Dataset profiles: {profiles_file}\")\n",
    "    logger.info(f\"Analysis report: {report_file}\")\n",
    "    \n",
    "    return results_file, summary_file, profiles_file, report_file\n",
    "\n",
    "# Save results\n",
    "if 'all_results' in locals() and 'report' in locals() and all_results:\n",
    "    results_file, summary_file, profiles_file, report_file = save_identification_results(identifier, all_results, report)\n",
    "    logger.info(\"All results saved successfully\")\n",
    "else:\n",
    "    logger.warning(\"Saving skipped - no data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093eb262-b9c2-4028-84f2-1ab625b17f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Summary\n",
    "\n",
    "if 'all_results' in locals() and 'report' in locals() and all_results:\n",
    "    logger.info(\"Quality identification analysis completed\")\n",
    "   \n",
    "    # Overall statistics\n",
    "    total_images = len(all_results)\n",
    "    flagged_images = len([r for r in all_results if r['recommended_action'] == 'REMOVE'])\n",
    "    removal_percentage = flagged_images / total_images * 100 if total_images > 0 else 0\n",
    "   \n",
    "    logger.info(f\"Overall Statistics:\")\n",
    "    logger.info(f\"   Total images analyzed: {total_images:,}\")\n",
    "    logger.info(f\"   Images flagged: {flagged_images:,}\")\n",
    "    logger.info(f\"   Removal rate: {removal_percentage:.1f}%\")\n",
    "   \n",
    "    # Per-dataset breakdown\n",
    "    logger.info(f\"Dataset breakdown:\")\n",
    "    dataset_stats = defaultdict(lambda: {'total': 0, 'flagged': 0})\n",
    "   \n",
    "    for result in all_results:\n",
    "        dataset = result['dataset']\n",
    "        dataset_stats[dataset]['total'] += 1\n",
    "        if result['recommended_action'] == 'REMOVE':\n",
    "            dataset_stats[dataset]['flagged'] += 1\n",
    "   \n",
    "    for dataset, stats in dataset_stats.items():\n",
    "        removal_rate = stats['flagged'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "        logger.info(f\"   {dataset}: {stats['flagged']:,}/{stats['total']:,} ({removal_rate:.1f}%)\")\n",
    "   \n",
    "    # Per-DR class breakdown\n",
    "    logger.info(f\"DR class breakdown:\")\n",
    "    dr_stats = defaultdict(lambda: {'total': 0, 'flagged': 0})\n",
    "   \n",
    "    for result in all_results:\n",
    "        dr_class = result['dr_severity']\n",
    "        dr_stats[dr_class]['total'] += 1\n",
    "        if result['recommended_action'] == 'REMOVE':\n",
    "            dr_stats[dr_class]['flagged'] += 1\n",
    "   \n",
    "    dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "    for dr_class in sorted(dr_stats.keys()):\n",
    "        stats = dr_stats[dr_class]\n",
    "        removal_rate = stats['flagged'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "        dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "        logger.info(f\"   {dr_name}: {stats['flagged']:,}/{stats['total']:,} ({removal_rate:.1f}%)\")\n",
    "   \n",
    "    logger.info(f\"Top removal reasons:\")\n",
    "    if 'removal_reasons_summary' in report:\n",
    "        sorted_reasons = sorted(report['removal_reasons_summary'].items(), key=lambda x: x[1], reverse=True)\n",
    "        for reason, count in sorted_reasons[:10]:\n",
    "            percentage = count / flagged_images * 100 if flagged_images > 0 else 0\n",
    "            logger.info(f\"   {reason}: {count:,} ({percentage:.1f}% of flagged images)\")\n",
    "   \n",
    "    # Recommendations\n",
    "    if 'recommendations' in report and report['recommendations']:\n",
    "        logger.info(f\"Recommendations:\")\n",
    "        for i, recommendation in enumerate(report['recommendations'], 1):\n",
    "            logger.info(f\"   {i}. {recommendation}\")\n",
    "   \n",
    "    # File locations\n",
    "    logger.info(f\"Results saved to:\")\n",
    "    logger.info(f\"   Flagged images summary: {OUTPUT_DIR}/flagged_images_summary.csv\")\n",
    "    logger.info(f\"   Detailed results: {OUTPUT_DIR}/quality_identification_results.csv\")\n",
    "    logger.info(f\"   Analysis report: {OUTPUT_DIR}/identification_report.json\")\n",
    "    logger.info(f\"   Sample images: {OUTPUT_DIR}/sample_images/\")\n",
    "    logger.info(f\"   Flagged samples: {OUTPUT_DIR}/flagged_samples/\")\n",
    "   \n",
    "    logger.info(f\"Quality identification process completed successfully\")\n",
    "else:\n",
    "    logger.error(\"No results available for summary - ensure previous analysis steps completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe342e61-aff3-42f7-afca-69cf8f233494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Visualization\n",
    "\n",
    "if 'all_results' in locals() and all_results:\n",
    "    logger.info(\"Creating analysis visualizations\")\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('DR Dataset Quality Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # 1. Removal rates by dataset\n",
    "    ax1 = axes[0, 0]\n",
    "    dataset_removal_rates = []\n",
    "    dataset_names = []\n",
    "    \n",
    "    for dataset in df['dataset'].unique():\n",
    "        dataset_data = df[df['dataset'] == dataset]\n",
    "        removal_rate = len(dataset_data[dataset_data['recommended_action'] == 'REMOVE']) / len(dataset_data) * 100\n",
    "        dataset_removal_rates.append(removal_rate)\n",
    "        dataset_names.append(dataset)\n",
    "    \n",
    "    bars1 = ax1.bar(range(len(dataset_names)), dataset_removal_rates, color='lightcoral', alpha=0.7)\n",
    "    ax1.set_xlabel('Dataset')\n",
    "    ax1.set_ylabel('Removal Rate (%)')\n",
    "    ax1.set_title('Removal Rates by Dataset')\n",
    "    ax1.set_xticks(range(len(dataset_names)))\n",
    "    ax1.set_xticklabels(dataset_names, rotation=45, ha='right')\n",
    "    \n",
    "    for bar, rate in zip(bars1, dataset_removal_rates):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Removal rates by DR class\n",
    "    ax2 = axes[0, 1]\n",
    "    dr_removal_rates = []\n",
    "    dr_labels = []\n",
    "    dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "    \n",
    "    for dr_class in sorted(df['dr_severity'].unique()):\n",
    "        dr_data = df[df['dr_severity'] == dr_class]\n",
    "        removal_rate = len(dr_data[dr_data['recommended_action'] == 'REMOVE']) / len(dr_data) * 100\n",
    "        dr_removal_rates.append(removal_rate)\n",
    "        dr_label = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "        dr_labels.append(dr_label)\n",
    "    \n",
    "    bars2 = ax2.bar(range(len(dr_labels)), dr_removal_rates, color='lightblue', alpha=0.7)\n",
    "    ax2.set_xlabel('DR Severity')\n",
    "    ax2.set_ylabel('Removal Rate (%)')\n",
    "    ax2.set_title('Removal Rates by DR Severity')\n",
    "    ax2.set_xticks(range(len(dr_labels)))\n",
    "    ax2.set_xticklabels(dr_labels, rotation=45, ha='right')\n",
    "    \n",
    "    for bar, rate in zip(bars2, dr_removal_rates):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Quality score distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    keep_scores = df[df['recommended_action'] == 'KEEP']['overall_quality_score']\n",
    "    remove_scores = df[df['recommended_action'] == 'REMOVE']['overall_quality_score']\n",
    "    ax3.hist(keep_scores, bins=30, alpha=0.7, label='Keep', color='lightgreen', density=True)\n",
    "    ax3.hist(remove_scores, bins=30, alpha=0.7, label='Remove', color='lightcoral', density=True)\n",
    "    ax3.set_xlabel('Quality Score')\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.set_title('Quality Score Distribution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Top removal reasons\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'removal_reasons_summary' in report:\n",
    "        reasons = list(report['removal_reasons_summary'].keys())[:10]\n",
    "        counts = [report['removal_reasons_summary'][reason] for reason in reasons]\n",
    "        \n",
    "        bars4 = ax4.barh(range(len(reasons)), counts, color='orange', alpha=0.7)\n",
    "        ax4.set_ylabel('Removal Reason')\n",
    "        ax4.set_xlabel('Count')\n",
    "        ax4.set_title('Top Removal Reasons')\n",
    "        ax4.set_yticks(range(len(reasons)))\n",
    "        ax4.set_yticklabels([reason.replace('_', ' ').title() for reason in reasons])\n",
    "        \n",
    "        for bar, count in zip(bars4, counts):\n",
    "            width = bar.get_width()\n",
    "            ax4.text(width + max(counts)*0.01, bar.get_y() + bar.get_height()/2.,\n",
    "                    f'{count}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_file = f'{OUTPUT_DIR}/analysis_summary_plots.png'\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(f\"Visualization saved to: {plot_file}\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Visualization creation skipped - no results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee55f3e-69d7-458f-8a35-ac29d1439a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Final Checkpoint - Verification\n",
    "\n",
    "# Final verification that all components completed successfully\n",
    "print(\"FINAL VERIFICATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if all major components completed\n",
    "checks = {\n",
    "    'Valid datasets found': 'valid_datasets' in locals() and bool(valid_datasets),\n",
    "    'Dataset profiles created': 'dataset_profiles' in locals() and bool(dataset_profiles),\n",
    "    'Quality analysis completed': 'all_results' in locals() and bool(all_results),\n",
    "    'Report generated': 'report' in locals() and bool(report),\n",
    "    'Results saved': os.path.exists(f'{OUTPUT_DIR}/flagged_images_summary.csv'),\n",
    "    'Sample images created': os.path.exists(f'{OUTPUT_DIR}/sample_images'),\n",
    "    'Flagged samples created': os.path.exists(f'{OUTPUT_DIR}/flagged_samples')\n",
    "}\n",
    "\n",
    "all_good = True\n",
    "for check_name, check_result in checks.items():\n",
    "    status = \"PASS\" if check_result else \"FAIL\"\n",
    "    print(f\"{status}: {check_name}\")\n",
    "    if not check_result:\n",
    "        all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(f\"\\nSUCCESS: All components completed successfully\")\n",
    "    print(f\"Check the '{OUTPUT_DIR}' directory for all results\")\n",
    "   \n",
    "    if 'all_results' in locals():\n",
    "        total = len(all_results)\n",
    "        flagged = len([r for r in all_results if r['recommended_action'] == 'REMOVE'])\n",
    "        print(f\"Final count: {flagged:,} of {total:,} images flagged for removal ({flagged/total*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Some components did not complete successfully\")\n",
    "    print(f\"Please review the error messages above and re-run the failed sections\")\n",
    "\n",
    "print(f\"\\nIMPORTANT: This analysis only IDENTIFIED potential issues\")\n",
    "print(f\"NO IMAGES WERE ACTUALLY REMOVED from your datasets\")\n",
    "print(f\"Review the flagged samples before deciding on actual removal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Check what sample images were saved\n",
    "import os\n",
    "sample_base_dir = f'{identifier.output_dir}/sample_images'\n",
    "print(\"Sample images saved:\")\n",
    "for dataset_name in os.listdir(sample_base_dir):\n",
    "    dataset_sample_dir = os.path.join(sample_base_dir, dataset_name)\n",
    "    if os.path.isdir(dataset_sample_dir):\n",
    "        num_samples = len([f for f in os.listdir(dataset_sample_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"   {dataset_name}: {num_samples} sample images in {dataset_sample_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Manual Threshold Adjustment (Optional)\n",
    "\n",
    "logger.info(\"Manual threshold adjustment interface\")\n",
    "\n",
    "if 'dataset_profiles' not in locals() or not dataset_profiles:\n",
    "    logger.error(\"Dataset profiles not found - run characterization first\")\n",
    "else:\n",
    "    logger.info(\"Current thresholds:\")\n",
    "    for dataset_name, profile in dataset_profiles.items():\n",
    "        logger.info(f\"{dataset_name}:\")\n",
    "        for dr_class, threshold in profile['adaptive_thresholds'].items():\n",
    "            dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "            dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "            logger.info(f\"  {dr_name} (Class {dr_class}): {threshold:.3f}\")\n",
    "\n",
    "    logger.info(\"Manual threshold override section\")\n",
    "    logger.info(\"Edit values below to adjust thresholds\")\n",
    "    logger.info(\"Set to None to keep current value\")\n",
    "\n",
    "    manual_thresholds = {}\n",
    "    for dataset_name in dataset_profiles.keys():\n",
    "        manual_thresholds[dataset_name] = {\n",
    "            0: None,  # Set to desired value like 0.250 or None to keep current\n",
    "            1: None,\n",
    "            2: None,\n",
    "            3: None,\n",
    "            4: None\n",
    "        }\n",
    "\n",
    "    updated_profiles = {}\n",
    "    changes_made = False\n",
    "\n",
    "    for dataset_name, profile in dataset_profiles.items():\n",
    "        updated_profiles[dataset_name] = profile.copy()\n",
    "        updated_profiles[dataset_name]['adaptive_thresholds'] = profile['adaptive_thresholds'].copy()\n",
    "        \n",
    "        if dataset_name in manual_thresholds:\n",
    "            for dr_class, manual_threshold in manual_thresholds[dataset_name].items():\n",
    "                if manual_threshold is not None:\n",
    "                    old_threshold = profile['adaptive_thresholds'][dr_class]\n",
    "                    updated_profiles[dataset_name]['adaptive_thresholds'][dr_class] = manual_threshold\n",
    "                    changes_made = True\n",
    "                    dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "                    dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "                    logger.info(f\"Updated {dataset_name} - {dr_name}: {old_threshold:.3f} → {manual_threshold:.3f}\")\n",
    "\n",
    "    if changes_made:\n",
    "        logger.info(\"Threshold adjustments applied\")\n",
    "        dataset_profiles = updated_profiles\n",
    "    else:\n",
    "        logger.info(\"No manual adjustments made - using original thresholds\")\n",
    "\n",
    "    logger.info(\"Final thresholds after adjustment:\")\n",
    "    for dataset_name, profile in dataset_profiles.items():\n",
    "        logger.info(f\"{dataset_name}:\")\n",
    "        for dr_class, threshold in profile['adaptive_thresholds'].items():\n",
    "            dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "            dr_name = dr_names[dr_class] if 0 <= dr_class < 5 else f'Class {dr_class}'\n",
    "            logger.info(f\"  {dr_name} (Class {dr_class}): {threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97871da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Dataset Cleaning with Standardized Quality Scoring\n",
    "\n",
    "def create_cleaned_dataset(source_datasets, profiles, copy_output_dir='DREAM_dataset_cleaned'):\n",
    "    \"\"\"\n",
    "    Create cleaned dataset using standardized 3-component quality scoring.\n",
    "    Components: Basic Quality (25%), Medical Quality (55%), Technical Quality (20%)\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"Creating cleaned dataset: {copy_output_dir}\")\n",
    "    logger.info(\"Quality scoring: CQ = 0.25*Basic + 0.55*Medical + 0.20*Technical\")\n",
    "    \n",
    "    os.makedirs(copy_output_dir, exist_ok=True)\n",
    "    \n",
    "    copy_stats = {\n",
    "        'total_processed': 0, 'total_kept': 0, 'total_removed': 0,\n",
    "        'by_dataset': {}, 'by_dr_class': {i: {'kept': 0, 'removed': 0} for i in range(5)}\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for dataset_name, dataset_path in source_datasets.items():\n",
    "        logger.info(f\"Processing {dataset_name}\")\n",
    "        \n",
    "        if dataset_name not in profiles:\n",
    "            logger.error(f\"Profile not found for {dataset_name}\")\n",
    "            continue\n",
    "        \n",
    "        profile = profiles[dataset_name]\n",
    "        thresholds = profile['adaptive_thresholds']\n",
    "        \n",
    "        copy_dataset_dir = os.path.join(copy_output_dir, dataset_name)\n",
    "        os.makedirs(copy_dataset_dir, exist_ok=True)\n",
    "        \n",
    "        copy_dataset_stats = {\n",
    "            'total_processed': 0, 'total_kept': 0, 'total_removed': 0,\n",
    "            'by_dr_class': {i: {'kept': 0, 'removed': 0} for i in range(5)}\n",
    "        }\n",
    "        \n",
    "        for dr_class in range(5):\n",
    "            dr_class_path = os.path.join(dataset_path, str(dr_class))\n",
    "            \n",
    "            if not os.path.exists(dr_class_path):\n",
    "                continue\n",
    "            \n",
    "            copy_dr_dir = os.path.join(copy_dataset_dir, str(dr_class))\n",
    "            os.makedirs(copy_dr_dir, exist_ok=True)\n",
    "            \n",
    "            image_files = [f for f in os.listdir(dr_class_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff', '.bmp'))]\n",
    "            \n",
    "            if not image_files:\n",
    "                continue\n",
    "                \n",
    "            logger.info(f\"DR Class {dr_class}: Processing {len(image_files)} images\")\n",
    "            \n",
    "            threshold = thresholds.get(dr_class, 0.3)\n",
    "            kept_count = 0\n",
    "            removed_count = 0\n",
    "            \n",
    "            for i, image_file in enumerate(image_files):\n",
    "                if i % 500 == 0 and i > 0:\n",
    "                    progress = (i / len(image_files)) * 100\n",
    "                    logger.info(f\"Progress: {progress:.1f}% ({i}/{len(image_files)})\")\n",
    "                \n",
    "                if i % 1000 == 0:\n",
    "                    gc.collect()\n",
    "                \n",
    "                try:\n",
    "                    source_image_path = os.path.join(dr_class_path, image_file)\n",
    "                    \n",
    "                    char = identifier.analyze_single_image(source_image_path)\n",
    "                    if not char:\n",
    "                        continue\n",
    "                    \n",
    "                    combined_quality = calculate_standardized_quality_score(char)\n",
    "                    \n",
    "                    if combined_quality >= threshold:\n",
    "                        copy_dest_path = os.path.join(copy_dr_dir, image_file)\n",
    "                        shutil.copy2(source_image_path, copy_dest_path)\n",
    "                        kept_count += 1\n",
    "                        copy_dataset_stats['by_dr_class'][dr_class]['kept'] += 1\n",
    "                        copy_stats['by_dr_class'][dr_class]['kept'] += 1\n",
    "                    else:\n",
    "                        removed_count += 1\n",
    "                        copy_dataset_stats['by_dr_class'][dr_class]['removed'] += 1\n",
    "                        copy_stats['by_dr_class'][dr_class]['removed'] += 1\n",
    "                    \n",
    "                    copy_dataset_stats['total_processed'] += 1\n",
    "                    copy_stats['total_processed'] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {image_file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            copy_dataset_stats['total_kept'] += kept_count\n",
    "            copy_dataset_stats['total_removed'] += removed_count\n",
    "            \n",
    "            if kept_count + removed_count > 0:\n",
    "                removal_rate = (removed_count / (kept_count + removed_count)) * 100\n",
    "                dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "                dr_name = dr_names[dr_class]\n",
    "                logger.info(f\"{dr_name}: Kept {kept_count}, Removed {removed_count} ({removal_rate:.1f}% removed)\")\n",
    "            \n",
    "            gc.collect()\n",
    "        \n",
    "        copy_stats['total_kept'] += copy_dataset_stats['total_kept']\n",
    "        copy_stats['total_removed'] += copy_dataset_stats['total_removed']\n",
    "        copy_stats['by_dataset'][dataset_name] = copy_dataset_stats\n",
    "        \n",
    "        if copy_dataset_stats['total_processed'] > 0:\n",
    "            dataset_removal_rate = (copy_dataset_stats['total_removed'] / copy_dataset_stats['total_processed']) * 100\n",
    "            logger.info(f\"{dataset_name} Summary: {copy_dataset_stats['total_processed']:,} processed, \"\n",
    "                       f\"{copy_dataset_stats['total_kept']:,} kept, \"\n",
    "                       f\"{copy_dataset_stats['total_removed']:,} removed ({dataset_removal_rate:.1f}%)\")\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    copy_stats_file = os.path.join(copy_output_dir, 'cleaning_statistics.json')\n",
    "    with open(copy_stats_file, 'w') as f:\n",
    "        json.dump(copy_stats, f, indent=2)\n",
    "    \n",
    "    return copy_stats, processing_time\n",
    "\n",
    "def calculate_standardized_quality_score(characteristics):\n",
    "    \"\"\"\n",
    "    Calculate standardized composite quality score using 3-component system.\n",
    "    Consistent with Cell 7 methodology.\n",
    "    \"\"\"\n",
    "    \n",
    "    brightness = float(characteristics.get('brightness', 127.5))\n",
    "    contrast = float(characteristics.get('contrast', 50.0))\n",
    "    sharpness = float(characteristics.get('sharpness', 500.0))\n",
    "    entropy = float(characteristics.get('entropy', 4.0))\n",
    "    \n",
    "    illumination_uniformity = float(characteristics.get('illumination_uniformity', 0.5))\n",
    "    vessel_visibility = float(characteristics.get('vessel_visibility', 0.1))\n",
    "    optic_disc_visibility = float(characteristics.get('optic_disc_visibility', 0.1))\n",
    "    \n",
    "    extreme_pixels = float(characteristics.get('extreme_brightness_pixels', 0.1))\n",
    "    motion_blur = float(characteristics.get('motion_blur_score', 20.0))\n",
    "    color_balance = float(characteristics.get('color_balance', 15.0))\n",
    "    \n",
    "    # Basic Quality Component normalization\n",
    "    brightness_norm = min(1.0, max(0.0, brightness / 255.0))\n",
    "    contrast_norm = min(1.0, max(0.0, contrast / 100.0))\n",
    "    sharpness_norm = min(1.0, max(0.0, sharpness / 1000.0))\n",
    "    entropy_norm = min(1.0, max(0.0, entropy / 8.0))\n",
    "    \n",
    "    basic_quality = np.mean([brightness_norm, contrast_norm, sharpness_norm, entropy_norm])\n",
    "    \n",
    "    # Medical Quality Component normalization\n",
    "    medical_quality = np.mean([\n",
    "        min(1.0, max(0.0, illumination_uniformity)),\n",
    "        min(1.0, max(0.0, vessel_visibility * 10)),\n",
    "        min(1.0, max(0.0, optic_disc_visibility * 10))\n",
    "    ])\n",
    "    \n",
    "    # Technical Quality Component normalization\n",
    "    extreme_pixels_norm = max(0, min(1, 1 - (extreme_pixels * 2)))\n",
    "    motion_blur_norm = min(1.0, max(0.0, motion_blur / 50.0))\n",
    "    color_balance_norm = max(0, min(1, 1 - (color_balance / 50.0)))\n",
    "    \n",
    "    technical_quality = np.mean([extreme_pixels_norm, motion_blur_norm, color_balance_norm])\n",
    "    \n",
    "    # Composite Quality Score: 25% Basic + 55% Medical + 20% Technical\n",
    "    return 0.25 * basic_quality + 0.55 * medical_quality + 0.20 * technical_quality\n",
    "\n",
    "def generate_cleaning_report(stats, processing_time, output_dir):\n",
    "    \"\"\"Generate cleaning summary report.\"\"\"\n",
    "    \n",
    "    total_processed = stats['total_processed']\n",
    "    total_kept = stats['total_kept']\n",
    "    total_removed = stats['total_removed']\n",
    "    \n",
    "    if total_processed == 0:\n",
    "        logger.warning(\"No images processed\")\n",
    "        return\n",
    "    \n",
    "    overall_removal_rate = (total_removed / total_processed) * 100\n",
    "    \n",
    "    logger.info(f\"Cleaning Summary:\")\n",
    "    logger.info(f\"Processing time: {processing_time/60:.1f} minutes\")\n",
    "    logger.info(f\"Total processed: {total_processed:,}\")\n",
    "    logger.info(f\"Images kept: {total_kept:,}\")\n",
    "    logger.info(f\"Images removed: {total_removed:,} ({overall_removal_rate:.1f}%)\")\n",
    "    \n",
    "    logger.info(f\"Dataset breakdown:\")\n",
    "    for dataset_name, dataset_stats in stats['by_dataset'].items():\n",
    "        if dataset_stats['total_processed'] > 0:\n",
    "            removal_rate = (dataset_stats['total_removed'] / dataset_stats['total_processed']) * 100\n",
    "            logger.info(f\"  {dataset_name}: {dataset_stats['total_kept']:,} kept, \"\n",
    "                       f\"{dataset_stats['total_removed']:,} removed ({removal_rate:.1f}%)\")\n",
    "    \n",
    "    logger.info(f\"DR class breakdown:\")\n",
    "    dr_names = ['No DR', 'Mild DR', 'Moderate DR', 'Severe DR', 'Proliferative DR']\n",
    "    for dr_class in range(5):\n",
    "        dr_stats = stats['by_dr_class'][dr_class]\n",
    "        total_dr = dr_stats['kept'] + dr_stats['removed']\n",
    "        if total_dr > 0:\n",
    "            removal_rate = (dr_stats['removed'] / total_dr) * 100\n",
    "            logger.info(f\"  {dr_names[dr_class]}: {dr_stats['kept']:,} kept, \"\n",
    "                       f\"{dr_stats['removed']:,} removed ({removal_rate:.1f}%)\")\n",
    "    \n",
    "    logger.info(f\"Output location: {output_dir}\")\n",
    "    logger.info(f\"Statistics saved: {output_dir}/cleaning_statistics.json\")\n",
    "\n",
    "# Execute dataset cleaning\n",
    "if 'valid_datasets' in locals() and 'dataset_profiles' in locals():\n",
    "    logger.info(\"Starting dataset cleaning with standardized quality scoring\")\n",
    "    \n",
    "    final_stats, total_processing_time = create_cleaned_dataset(\n",
    "        valid_datasets, dataset_profiles\n",
    "    )\n",
    "    \n",
    "    generate_cleaning_report(final_stats, total_processing_time, 'DREAM_dataset_cleaned')\n",
    "    \n",
    "    logger.info(\"Dataset cleaning completed successfully\")\n",
    "else:\n",
    "    logger.error(\"Cannot proceed - missing required variables (valid_datasets, dataset_profiles)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ce149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Quality Analysis CSV Generation\n",
    "\n",
    "def create_quality_csv(all_results, output_file='quality_analysis.csv'):\n",
    "    \"\"\"Generate comprehensive quality analysis CSV with all metrics and assessments.\"\"\"\n",
    "    \n",
    "    dr_names = {0: 'No DR', 1: 'Mild DR', 2: 'Moderate DR', 3: 'Severe DR', 4: 'Proliferative DR'}\n",
    "    analysis_data = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        width, height = result.get('resolution', (0, 0))\n",
    "        \n",
    "        removal_reasons = result.get('removal_reasons', [])\n",
    "        if isinstance(removal_reasons, list):\n",
    "            removal_reasons_str = ';'.join(removal_reasons) if removal_reasons else ''\n",
    "        else:\n",
    "            removal_reasons_str = str(removal_reasons)\n",
    "        \n",
    "        analysis_record = {\n",
    "            'dataset_name': result.get('dataset', ''),\n",
    "            'filename': result.get('filename', ''),\n",
    "            'dr_severity': result.get('dr_severity', 0),\n",
    "            'dr_class_name': dr_names.get(result.get('dr_severity', 0), 'Unknown'),\n",
    "            \n",
    "            'overall_quality_score': round(result.get('overall_quality_score', 0), 4),\n",
    "            'basic_quality_score': round(result.get('basic_quality_score', 0), 4),\n",
    "            'medical_quality_score': round(result.get('medical_quality_score', 0), 4),\n",
    "            'technical_quality_score': round(result.get('technical_quality_score', 0), 4),\n",
    "            \n",
    "            'brightness': round(result.get('brightness', 0), 2),\n",
    "            'contrast': round(result.get('contrast', 0), 2),\n",
    "            'sharpness': round(result.get('sharpness', 0), 1),\n",
    "            'entropy': round(result.get('entropy', 0), 3),\n",
    "            'illumination_uniformity': round(result.get('illumination_uniformity', 0), 4),\n",
    "            \n",
    "            'vessel_visibility': round(result.get('vessel_visibility', 0), 4),\n",
    "            'optic_disc_visibility': round(result.get('optic_disc_visibility', 0), 4),\n",
    "            'color_balance': round(result.get('color_balance', 0), 2),\n",
    "            \n",
    "            'recommended_action': result.get('recommended_action', 'UNKNOWN'),\n",
    "            'confidence': result.get('confidence', 'UNKNOWN'),\n",
    "            'removal_reasons': removal_reasons_str,\n",
    "            'threshold_used': round(result.get('threshold_used', 0), 4),\n",
    "            \n",
    "            'image_width': width,\n",
    "            'image_height': height,\n",
    "            'file_size_mb': round(result.get('file_size_mb', 0), 2),\n",
    "            'extreme_brightness_pixels': round(result.get('extreme_brightness_pixels', 0), 4),\n",
    "            'motion_blur_score': round(result.get('motion_blur_score', 0), 2)\n",
    "        }\n",
    "        \n",
    "        analysis_data.append(analysis_record)\n",
    "    \n",
    "    df = pd.DataFrame(analysis_data)\n",
    "    df = df.sort_values(['dataset_name', 'dr_severity', 'overall_quality_score'], \n",
    "                       ascending=[True, True, False])\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    logger.info(f\"Quality analysis CSV saved: {output_file} ({len(df):,} records)\")\n",
    "    return df\n",
    "\n",
    "def create_summary_statistics(df, output_file='summary_statistics.csv'):\n",
    "    \"\"\"Generate comprehensive summary statistics from quality analysis data.\"\"\"\n",
    "    \n",
    "    total_images = len(df)\n",
    "    images_kept = len(df[df['recommended_action'] == 'KEEP'])\n",
    "    images_removed = len(df[df['recommended_action'] == 'REMOVE'])\n",
    "    removal_rate = round(images_removed / total_images * 100, 1)\n",
    "    \n",
    "    overall_stats = {\n",
    "        'Metric': ['Total Images', 'Images Kept', 'Images Removed', 'Removal Rate (%)'],\n",
    "        'Value': [total_images, images_kept, images_removed, removal_rate]\n",
    "    }\n",
    "    \n",
    "    dataset_stats = []\n",
    "    for dataset in df['dataset_name'].unique():\n",
    "        dataset_data = df[df['dataset_name'] == dataset]\n",
    "        removed = len(dataset_data[dataset_data['recommended_action'] == 'REMOVE'])\n",
    "        total = len(dataset_data)\n",
    "        \n",
    "        dataset_stats.append({\n",
    "            'Dataset': dataset,\n",
    "            'Total_Images': total,\n",
    "            'Images_Kept': total - removed,\n",
    "            'Images_Removed': removed,\n",
    "            'Removal_Rate_Percent': round(removed / total * 100, 1),\n",
    "            'Mean_Quality_Score': round(dataset_data['overall_quality_score'].mean(), 3),\n",
    "            'Std_Quality_Score': round(dataset_data['overall_quality_score'].std(), 3)\n",
    "        })\n",
    "    \n",
    "    dr_stats = []\n",
    "    for dr_class in sorted(df['dr_severity'].unique()):\n",
    "        dr_data = df[df['dr_severity'] == dr_class]\n",
    "        removed = len(dr_data[dr_data['recommended_action'] == 'REMOVE'])\n",
    "        total = len(dr_data)\n",
    "        \n",
    "        dr_stats.append({\n",
    "            'DR_Severity': dr_class,\n",
    "            'DR_Class_Name': dr_data['dr_class_name'].iloc[0],\n",
    "            'Total_Images': total,\n",
    "            'Images_Kept': total - removed,\n",
    "            'Images_Removed': removed,\n",
    "            'Removal_Rate_Percent': round(removed / total * 100, 1),\n",
    "            'Mean_Quality_Score': round(dr_data['overall_quality_score'].mean(), 3),\n",
    "            'Std_Quality_Score': round(dr_data['overall_quality_score'].std(), 3)\n",
    "        })\n",
    "    \n",
    "    quality_metrics = ['brightness', 'contrast', 'sharpness', 'entropy', \n",
    "                      'illumination_uniformity', 'vessel_visibility', 'optic_disc_visibility']\n",
    "    \n",
    "    metric_stats = []\n",
    "    for metric in quality_metrics:\n",
    "        if metric in df.columns:\n",
    "            metric_stats.append({\n",
    "                'Metric': metric,\n",
    "                'Mean': round(df[metric].mean(), 4),\n",
    "                'Std': round(df[metric].std(), 4),\n",
    "                'Min': round(df[metric].min(), 4),\n",
    "                'Max': round(df[metric].max(), 4),\n",
    "                'Q25': round(df[metric].quantile(0.25), 4),\n",
    "                'Q50': round(df[metric].quantile(0.50), 4),\n",
    "                'Q75': round(df[metric].quantile(0.75), 4)\n",
    "            })\n",
    "    \n",
    "    output_excel = output_file.replace('.csv', '.xlsx')\n",
    "    \n",
    "    try:\n",
    "        with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:\n",
    "            pd.DataFrame(overall_stats).to_excel(writer, sheet_name='Overall_Stats', index=False)\n",
    "            pd.DataFrame(dataset_stats).to_excel(writer, sheet_name='Dataset_Stats', index=False)\n",
    "            pd.DataFrame(dr_stats).to_excel(writer, sheet_name='DR_Severity_Stats', index=False)\n",
    "            pd.DataFrame(metric_stats).to_excel(writer, sheet_name='Quality_Metrics_Stats', index=False)\n",
    "        \n",
    "        logger.info(f\"Summary statistics saved: {output_excel}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        base_name = output_file.replace('.csv', '')\n",
    "        pd.DataFrame(overall_stats).to_csv(f'{base_name}_overall.csv', index=False)\n",
    "        pd.DataFrame(dataset_stats).to_csv(f'{base_name}_datasets.csv', index=False)\n",
    "        pd.DataFrame(dr_stats).to_csv(f'{base_name}_dr_severity.csv', index=False)\n",
    "        pd.DataFrame(metric_stats).to_csv(f'{base_name}_quality_metrics.csv', index=False)\n",
    "        logger.info(f\"Summary statistics saved as separate CSV files\")\n",
    "    \n",
    "    return dataset_stats, dr_stats, metric_stats\n",
    "\n",
    "def generate_analysis_files():\n",
    "    \"\"\"Main function to generate quality analysis files.\"\"\"\n",
    "    \n",
    "    if 'all_results' not in locals() and 'all_results' not in globals():\n",
    "        logger.error(\"Quality analysis results not found\")\n",
    "        return None\n",
    "    \n",
    "    results = globals().get('all_results', locals().get('all_results', []))\n",
    "    \n",
    "    if not results:\n",
    "        logger.error(\"No analysis results available\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Processing {len(results):,} quality analysis results\")\n",
    "    \n",
    "    analysis_df = create_quality_csv(results, 'quality_analysis.csv')\n",
    "    \n",
    "    dataset_stats, dr_stats, metric_stats = create_summary_statistics(\n",
    "        analysis_df, 'summary_statistics.csv'\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Dataset Summary:\")\n",
    "    for stat in dataset_stats:\n",
    "        logger.info(f\"  {stat['Dataset']}: {stat['Total_Images']:,} images, \"\n",
    "                   f\"{stat['Removal_Rate_Percent']}% removed\")\n",
    "    \n",
    "    logger.info(f\"DR Severity Summary:\")\n",
    "    for stat in dr_stats:\n",
    "        logger.info(f\"  {stat['DR_Class_Name']}: {stat['Total_Images']:,} images, \"\n",
    "                   f\"{stat['Removal_Rate_Percent']}% removed\")\n",
    "    \n",
    "    logger.info(f\"Files generated:\")\n",
    "    logger.info(f\"- quality_analysis.csv: Complete analysis dataset\")\n",
    "    logger.info(f\"- summary_statistics.xlsx: Statistical summaries\")\n",
    "    \n",
    "    return analysis_df, dataset_stats, dr_stats, metric_stats\n",
    "\n",
    "# Execute analysis file generation\n",
    "if 'all_results' in locals() and all_results:\n",
    "    generate_analysis_files()\n",
    "else:\n",
    "    logger.warning(\"Analysis file generation skipped - no results available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
